{
    "type": "dir", 
    "name": "~", 
    "contents": [
        {
            "type": "text", 
            "name": "CHANGES.md", 
            "contents": "### 0.4.6\n(May 21st, 2014)\n\n- Raise a warning when RQ workers are used with Sentry DSNs using\n  asynchronous transports.  Thanks Wei, Selwin & Toms!\n\n\n### 0.4.5\n(May 8th, 2014)\n\n- Fix where rqworker broke on Python 2.6. Thanks, Marko!\n\n\n### 0.4.4\n(May 7th, 2014)\n\n- Properly declare redis dependency.\n- Fix a NameError regression that was introduced in 0.4.3.\n\n\n### 0.4.3\n(May 6th, 2014)\n\n- Make job and queue classes overridable. Thanks, Marko!\n- Don't require connection for @job decorator at definition time. Thanks, Sasha!\n- Syntactic code cleanup.\n\n\n### 0.4.2\n(April 28th, 2014)\n\n- Add missing depends_on kwarg to @job decorator.  Thanks, Sasha!\n\n\n### 0.4.1\n(April 22nd, 2014)\n\n- Fix bug where RQ 0.4 workers could not unpickle/process jobs from RQ < 0.4.\n\n\n### 0.4.0\n(April 22nd, 2014)\n\n- Emptying the failed queue from the command line is now as simple as running\n  `rqinfo -X` or `rqinfo --empty-failed-queue`.\n\n- Job data is unpickled lazily. Thanks, Malthe!\n\n- Removed dependency on the `times` library. Thanks, Malthe!\n\n- Job dependencies!  Thanks, Selwin.\n\n- Custom worker classes, via the `--worker-class=path.to.MyClass` command line\n  argument.  Thanks, Selwin.\n\n- `Queue.all()` and `rqinfo` now report empty queues, too.  Thanks, Rob!\n\n- Fixed a performance issue in `Queue.all()` when issued in large Redis DBs.\n  Thanks, Rob!\n\n- Birth and death dates are now stored as proper datetimes, not timestamps.\n\n- Ability to provide a custom job description (instead of using the default\n  function invocation hint).  Thanks, \u0130brahim.\n\n- Fix: temporary key for the compact queue is now randomly generated, which\n  should avoid name clashes for concurrent compact actions.\n\n- Fix: `Queue.empty()` now correctly deletes job hashes from Redis.\n\n\n### 0.3.13\n(December 17th, 2013)\n\n- Bug fix where the worker crashes on jobs that have their timeout explicitly\n  removed.  Thanks for reporting, @algrs.\n\n\n### 0.3.12\n(December 16th, 2013)\n\n- Bug fix where a worker could time out before the job was done, removing it\n  from any monitor overviews (#288).\n\n\n### 0.3.11\n(August 23th, 2013)\n\n- Some more fixes in command line scripts for Python 3\n\n\n### 0.3.10\n(August 20th, 2013)\n\n- Bug fix in setup.py\n\n\n### 0.3.9\n(August 20th, 2013)\n\n- Python 3 compatibility (Thanks, Alex!)\n\n- Minor bug fix where Sentry would break when func cannot be imported\n\n\n### 0.3.8\n(June 17th, 2013)\n\n- `rqworker` and `rqinfo` have a  `--url` argument to connect to a Redis url.\n\n- `rqworker` and `rqinfo` have a `--socket` option to connect to a Redis server\n  through a Unix socket.\n\n- `rqworker` reads `SENTRY_DSN` from the environment, unless specifically\n  provided on the command line.\n\n- `Queue` has a new API that supports paging `get_jobs(3, 7)`, which will\n  return at most 7 jobs, starting from the 3rd.\n\n\n### 0.3.7\n(February 26th, 2013)\n\n- Fixed bug where workers would not execute builtin functions properly.\n\n\n### 0.3.6\n(February 18th, 2013)\n\n- Worker registrations now expire.  This should prevent `rqinfo` from reporting\n  about ghosted workers.  (Thanks, @yaniv-aknin!)\n\n- `rqworker` will automatically clean up ghosted worker registrations from\n  pre-0.3.6 runs.\n\n- `rqworker` grew a `-q` flag, to be more silent (only warnings/errors are shown)\n\n\n### 0.3.5\n(February 6th, 2013)\n\n- `ended_at` is now recorded for normally finished jobs, too.  (Previously only\n  for failed jobs.)\n\n- Adds support for both `Redis` and `StrictRedis` connection types\n\n- Makes `StrictRedis` the default connection type if none is explicitly provided\n\n\n### 0.3.4\n(January 23rd, 2013)\n\n- Restore compatibility with Python 2.6.\n\n\n### 0.3.3\n(January 18th, 2013)\n\n- Fix bug where work was lost due to silently ignored unpickle errors.\n\n- Jobs can now access the current `Job` instance from within.  Relevant\n  documentation [here](http://python-rq.org/docs/jobs/).\n\n- Custom properties can be set by modifying the `job.meta` dict.  Relevant\n  documentation [here](http://python-rq.org/docs/jobs/).\n\n- Custom properties can be set by modifying the `job.meta` dict.  Relevant\n  documentation [here](http://python-rq.org/docs/jobs/).\n\n- `rqworker` now has an optional `--password` flag.\n\n- Remove `logbook` dependency (in favor of `logging`)\n\n\n### 0.3.2\n(September 3rd, 2012)\n\n- Fixes broken `rqinfo` command.\n\n- Improve compatibility with Python < 2.7.\n\n\n\n### 0.3.1\n(August 30th, 2012)\n\n- `.enqueue()` now takes a `result_ttl` keyword argument that can be used to\n  change the expiration time of results.\n\n- Queue constructor now takes an optional `async=False` argument to bypass the\n  worker (for testing purposes).\n\n- Jobs now carry status information.  To get job status information, like\n  whether a job is queued, finished, or failed, use the property `status`, or\n  one of the new boolean accessor properties `is_queued`, `is_finished` or\n  `is_failed`.\n\n- Jobs return values are always stored explicitly, even if they have to\n  explicit return value or return `None` (with given TTL of course).  This\n  makes it possible to distinguish between a job that explicitly returned\n  `None` and a job that isn't finished yet (see `status` property).\n\n- Custom exception handlers can now be configured in addition to, or to fully\n  replace, moving failed jobs to the failed queue.  Relevant documentation\n  [here](http://python-rq.org/docs/exceptions/) and\n  [here](http://python-rq.org/patterns/sentry/).\n\n- `rqworker` now supports passing in configuration files instead of the\n  many command line options: `rqworker -c settings` will source\n  `settings.py`.\n\n- `rqworker` now supports one-flag setup to enable Sentry as its exception\n  handler: `rqworker --sentry-dsn=\"http://public:secret@example.com/1\"`\n  Alternatively, you can use a settings file and configure `SENTRY_DSN\n  = 'http://public:secret@example.com/1'` instead.\n\n\n### 0.3.0\n(August 5th, 2012)\n\n- Reliability improvements\n\n    - Warm shutdown now exits immediately when Ctrl+C is pressed and worker is idle\n    - Worker does not leak worker registrations anymore when stopped gracefully\n\n- `.enqueue()` does not consume the `timeout` kwarg anymore.  Instead, to pass\n  RQ a timeout value while enqueueing a function, use the explicit invocation\n  instead:\n\n      ```python\n      q.enqueue(do_something, args=(1, 2), kwargs={'a': 1}, timeout=30)\n      ```\n\n- Add a `@job` decorator, which can be used to do Celery-style delayed\n  invocations:\n\n      ```python\n      from redis import StrictRedis\n      from rq.decorators import job\n\n      # Connect to Redis\n      redis = StrictRedis()\n\n      @job('high', timeout=10, connection=redis)\n      def some_work(x, y):\n          return x + y\n      ```\n\n  Then, in another module, you can call `some_work`:\n\n      ```python\n      from foo.bar import some_work\n\n      some_work.delay(2, 3)\n      ```\n\n\n### 0.2.2\n(August 1st, 2012)\n\n- Fix bug where return values that couldn't be pickled crashed the worker\n\n\n### 0.2.1\n(July 20th, 2012)\n\n- Fix important bug where result data wasn't restored from Redis correctly\n  (affected non-string results only).\n\n\n### 0.2.0\n(July 18th, 2012)\n\n- `q.enqueue()` accepts instance methods now, too.  Objects will be pickle'd\n  along with the instance method, so beware.\n- `q.enqueue()` accepts string specification of functions now, too.  Example:\n  `q.enqueue(\"my.math.lib.fibonacci\", 5)`.  Useful if the worker and the\n  submitter of work don't share code bases.\n- Job can be assigned custom attrs and they will be pickle'd along with the\n  rest of the job's attrs.  Can be used when writing RQ extensions.\n- Workers can now accept explicit connections, like Queues.\n- Various bug fixes.\n\n\n### 0.1.2\n(May 15, 2012)\n\n- Fix broken PyPI deployment.\n\n\n### 0.1.1\n(May 14, 2012)\n\n- Thread-safety by using context locals\n- Register scripts as console_scripts, for better portability\n- Various bugfixes.\n\n\n### 0.1.0:\n(March 28, 2012)\n\n- Initially released version.\n"
        }, 
        {
            "type": "text", 
            "name": "dev-requirements.txt", 
            "contents": "mock\n"
        }, 
        {
            "type": "dir", 
            "name": "examples", 
            "contents": [
                {
                    "type": "text", 
                    "name": "fib.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\n\ndef slow_fib(n):\n    if n <= 1:\n        return 1\n    else:\n        return slow_fib(n-1) + slow_fib(n-2)\n"
                }, 
                {
                    "type": "text", 
                    "name": "run_example.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport os\nimport time\n\nfrom rq import Connection, Queue\n\nfrom fib import slow_fib\n\n\ndef main():\n    # Range of Fibonacci numbers to compute\n    fib_range = range(20, 34)\n\n    # Kick off the tasks asynchronously\n    async_results = {}\n    q = Queue()\n    for x in fib_range:\n        async_results[x] = q.enqueue(slow_fib, x)\n\n    start_time = time.time()\n    done = False\n    while not done:\n        os.system('clear')\n        print('Asynchronously: (now = %.2f)' % (time.time() - start_time,))\n        done = True\n        for x in fib_range:\n            result = async_results[x].return_value\n            if result is None:\n                done = False\n                result = '(calculating)'\n            print('fib(%d) = %s' % (x, result))\n        print('')\n        print('To start the actual in the background, run a worker:')\n        print('    python examples/run_worker.py')\n        time.sleep(0.2)\n\n    print('Done')\n\n\nif __name__ == '__main__':\n    # Tell RQ what Redis connection to use\n    with Connection():\n        main()\n"
                }, 
                {
                    "type": "text", 
                    "name": "run_worker.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom rq import Connection, Queue, Worker\n\nif __name__ == '__main__':\n    # Tell rq what Redis connection to use\n    with Connection():\n        q = Queue()\n        Worker(q).work()\n"
                }
            ]
        }, 
        {
            "type": "text", 
            "name": "LICENSE", 
            "contents": "Copyright 2012 Vincent Driessen. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n   1. Redistributions of source code must retain the above copyright notice,\n      this list of conditions and the following disclaimer.\n\n   2. Redistributions in binary form must reproduce the above copyright notice,\n      this list of conditions and the following disclaimer in the documentation\n      and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY VINCENT DRIESSEN ``AS IS'' AND ANY EXPRESS OR\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT\nSHALL VINCENT DRIESSEN OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\nINCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\nOR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\nADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nThe views and conclusions contained in the software and documentation are those\nof the authors and should not be interpreted as representing official policies,\neither expressed or implied, of Vincent Driessen.\n"
        }, 
        {
            "type": "text", 
            "name": "MANIFEST.in", 
            "contents": "recursive-exclude tests *\n"
        }, 
        {
            "type": "text", 
            "name": "py26-requirements.txt", 
            "contents": "unittest2\nimportlib\nargparse\n"
        }, 
        {
            "type": "text", 
            "name": "README.md", 
            "contents": "[![Build status](https://secure.travis-ci.org/nvie/rq.png?branch=master)](https://secure.travis-ci.org/nvie/rq)\n\nRQ (_Redis Queue_) is a simple Python library for queueing jobs and processing\nthem in the background with workers.  It is backed by Redis and it is designed\nto have a low barrier to entry.  It should be integrated in your web stack\neasily.\n\n\n## Getting started\n\nFirst, run a Redis server, of course:\n\n```console\n$ redis-server\n```\n\nTo put jobs on queues, you don't have to do anything special, just define\nyour typically lengthy or blocking function:\n\n```python\nimport requests\n\ndef count_words_at_url(url):\n    \"\"\"Just an example function that's called async.\"\"\"\n    resp = requests.get(url)\n    return len(resp.text.split())\n```\n\nYou do use the excellent [requests][r] package, don't you?\n\nThen, create a RQ queue:\n\n```python\nfrom rq import Queue, use_connection\nuse_connection()\nq = Queue()\n```\n\nAnd enqueue the function call:\n\n```python\nfrom my_module import count_words_at_url\nresult = q.enqueue(count_words_at_url, 'http://nvie.com')\n```\n\nFor a more complete example, refer to the [docs][d].  But this is the essence.\n\n\n### The worker\n\nTo start executing enqueued function calls in the background, start a worker\nfrom your project's directory:\n\n```console\n$ rqworker\n*** Listening for work on default\nGot count_words_at_url('http://nvie.com') from default\nJob result = 818\n*** Listening for work on default\n```\n\nThat's about it.\n\n\n## Installation\n\nSimply use the following command to install the latest released version:\n\n    pip install rq\n\nIf you want the cutting edge version (that may well be broken), use this:\n\n    pip install -e git+git@github.com:nvie/rq.git@master#egg=rq\n\n\n## Project history\n\nThis project has been inspired by the good parts of [Celery][1], [Resque][2]\nand [this snippet][3], and has been created as a lightweight alternative to the\nheaviness of Celery or other AMQP-based queueing implementations.\n\n[r]: http://python-requests.org\n[d]: http://nvie.github.com/rq/docs/\n[m]: http://pypi.python.org/pypi/mailer\n[p]: http://docs.python.org/library/pickle.html\n[1]: http://www.celeryproject.org/\n[2]: https://github.com/defunkt/resque\n[3]: http://flask.pocoo.org/snippets/73/\n"
        }, 
        {
            "type": "text", 
            "name": "requirements.txt", 
            "contents": "redis\n"
        }, 
        {
            "type": "dir", 
            "name": "rq", 
            "contents": [
                {
                    "type": "text", 
                    "name": "__init__.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom .connections import (Connection, get_current_connection, pop_connection,\n                          push_connection, use_connection)\nfrom .job import cancel_job, get_current_job, requeue_job\nfrom .queue import get_failed_queue, Queue\nfrom .version import VERSION\nfrom .worker import Worker\n\n__all__ = [\n    'use_connection', 'get_current_connection',\n    'push_connection', 'pop_connection', 'Connection',\n    'Queue', 'get_failed_queue', 'Worker',\n    'cancel_job', 'requeue_job', 'get_current_job']\n__version__ = VERSION\n"
                }, 
                {
                    "type": "dir", 
                    "name": "compat", 
                    "contents": [
                        {
                            "type": "text", 
                            "name": "__init__.py", 
                            "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport sys\n\n\ndef is_python_version(*versions):\n    for version in versions:\n        if (sys.version_info[0] == version[0] and\n                sys.version_info >= version):\n            return True\n    return False\n\n\n# functools.total_ordering is only available from Python 2.7 and 3.2\nif is_python_version((2, 7), (3, 2)):\n    from functools import total_ordering\nelse:\n    def total_ordering(cls):  # noqa\n        \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n        convert = {\n            '__lt__': [('__gt__', lambda self, other: other < self),\n                       ('__le__', lambda self, other: not other < self),\n                       ('__ge__', lambda self, other: not self < other)],\n            '__le__': [('__ge__', lambda self, other: other <= self),\n                       ('__lt__', lambda self, other: not other <= self),\n                       ('__gt__', lambda self, other: not self <= other)],\n            '__gt__': [('__lt__', lambda self, other: other > self),\n                       ('__ge__', lambda self, other: not other > self),\n                       ('__le__', lambda self, other: not self > other)],\n            '__ge__': [('__le__', lambda self, other: other >= self),\n                       ('__gt__', lambda self, other: not other >= self),\n                       ('__lt__', lambda self, other: not self >= other)]\n        }\n        roots = set(dir(cls)) & set(convert)\n        if not roots:\n            raise ValueError('must define at least one ordering operation: < > <= >=')  # noqa\n        root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n        for opname, opfunc in convert[root]:\n            if opname not in roots:\n                opfunc.__name__ = str(opname)\n                opfunc.__doc__ = getattr(int, opname).__doc__\n                setattr(cls, opname, opfunc)\n        return cls\n\n\nPY2 = sys.version_info[0] == 2\nif not PY2:\n    # Python 3.x and up\n    text_type = str\n    string_types = (str,)\n\n    def as_text(v):\n        if v is None:\n            return None\n        elif isinstance(v, bytes):\n            return v.decode('utf-8')\n        elif isinstance(v, str):\n            return v\n        else:\n            raise ValueError('Unknown type %r' % type(v))\n\n    def decode_redis_hash(h):\n        return dict((as_text(k), h[k]) for k in h)\nelse:\n    # Python 2.x\n    text_type = unicode\n    string_types = (str, unicode)\n\n    def as_text(v):\n        return v\n\n    def decode_redis_hash(h):\n        return h\n"
                        }, 
                        {
                            "type": "text", 
                            "name": "connections.py", 
                            "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom functools import partial\n\nfrom redis import Redis, StrictRedis\n\n\ndef fix_return_type(func):\n    # deliberately no functools.wraps() call here, since the function being\n    # wrapped is a partial, which has no module\n    def _inner(*args, **kwargs):\n        value = func(*args, **kwargs)\n        if value is None:\n            value = -1\n        return value\n    return _inner\n\n\ndef patch_connection(connection):\n    if not isinstance(connection, StrictRedis):\n        raise ValueError('A StrictRedis or Redis connection is required.')\n\n    # Don't patch already patches objects\n    PATCHED_METHODS = ['_setex', '_lrem', '_zadd', '_pipeline', '_ttl']\n    if all([hasattr(connection, attr) for attr in PATCHED_METHODS]):\n        return connection\n\n    if isinstance(connection, Redis):\n        connection._setex = partial(StrictRedis.setex, connection)\n        connection._lrem = partial(StrictRedis.lrem, connection)\n        connection._zadd = partial(StrictRedis.zadd, connection)\n        connection._pipeline = partial(StrictRedis.pipeline, connection)\n        connection._ttl = fix_return_type(partial(StrictRedis.ttl, connection))\n        if hasattr(connection, 'pttl'):\n            connection._pttl = fix_return_type(partial(StrictRedis.pttl, connection))\n    elif isinstance(connection, StrictRedis):\n        connection._setex = connection.setex\n        connection._lrem = connection.lrem\n        connection._zadd = connection.zadd\n        connection._pipeline = connection.pipeline\n        connection._ttl = connection.ttl\n        if hasattr(connection, 'pttl'):\n            connection._pttl = connection.pttl\n    else:\n        raise ValueError('Unanticipated connection type: {}. Please report this.'.format(type(connection)))\n\n    return connection\n"
                        }, 
                        {
                            "type": "text", 
                            "name": "dictconfig.py", 
                            "contents": "# This is a copy of the Python logging.config.dictconfig module.  It is\n# provided here for backwards compatibility for Python versions prior to 2.7.\n#\n# Copyright 2009-2010 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nimport logging.handlers\nimport re\nimport sys\nimport types\nfrom rq.compat import string_types\n\nIDENTIFIER = re.compile('^[a-z_][a-z0-9_]*$', re.I)\n\ndef valid_ident(s):\n    m = IDENTIFIER.match(s)\n    if not m:\n        raise ValueError('Not a valid Python identifier: %r' % s)\n    return True\n\n#\n# This function is defined in logging only in recent versions of Python\n#\ntry:\n    from logging import _checkLevel\nexcept ImportError:\n    def _checkLevel(level):\n        if isinstance(level, int):\n            rv = level\n        elif str(level) == level:\n            if level not in logging._levelNames:\n                raise ValueError('Unknown level: %r' % level)\n            rv = logging._levelNames[level]\n        else:\n            raise TypeError('Level not an integer or a '\n                            'valid string: %r' % level)\n        return rv\n\n# The ConvertingXXX classes are wrappers around standard Python containers,\n# and they serve to convert any suitable values in the container. The\n# conversion converts base dicts, lists and tuples to their wrapped\n# equivalents, whereas strings which match a conversion format are converted\n# appropriately.\n#\n# Each wrapper should have a configurator attribute holding the actual\n# configurator to use for conversion.\n\nclass ConvertingDict(dict):\n    \"\"\"A converting dictionary wrapper.\"\"\"\n\n    def __getitem__(self, key):\n        value = dict.__getitem__(self, key)\n        result = self.configurator.convert(value)\n        #If the converted value is different, save for next time\n        if value is not result:\n            self[key] = result\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n                result.key = key\n        return result\n\n    def get(self, key, default=None):\n        value = dict.get(self, key, default)\n        result = self.configurator.convert(value)\n        #If the converted value is different, save for next time\n        if value is not result:\n            self[key] = result\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n                result.key = key\n        return result\n\n    def pop(self, key, default=None):\n        value = dict.pop(self, key, default)\n        result = self.configurator.convert(value)\n        if value is not result:\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n                result.key = key\n        return result\n\nclass ConvertingList(list):\n    \"\"\"A converting list wrapper.\"\"\"\n    def __getitem__(self, key):\n        value = list.__getitem__(self, key)\n        result = self.configurator.convert(value)\n        #If the converted value is different, save for next time\n        if value is not result:\n            self[key] = result\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n                result.key = key\n        return result\n\n    def pop(self, idx=-1):\n        value = list.pop(self, idx)\n        result = self.configurator.convert(value)\n        if value is not result:\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n        return result\n\nclass ConvertingTuple(tuple):\n    \"\"\"A converting tuple wrapper.\"\"\"\n    def __getitem__(self, key):\n        value = tuple.__getitem__(self, key)\n        result = self.configurator.convert(value)\n        if value is not result:\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n                result.key = key\n        return result\n\nclass BaseConfigurator(object):\n    \"\"\"\n    The configurator base class which defines some useful defaults.\n    \"\"\"\n\n    CONVERT_PATTERN = re.compile(r'^(?P<prefix>[a-z]+)://(?P<suffix>.*)$')\n\n    WORD_PATTERN = re.compile(r'^\\s*(\\w+)\\s*')\n    DOT_PATTERN = re.compile(r'^\\.\\s*(\\w+)\\s*')\n    INDEX_PATTERN = re.compile(r'^\\[\\s*(\\w+)\\s*\\]\\s*')\n    DIGIT_PATTERN = re.compile(r'^\\d+$')\n\n    value_converters = {\n        'ext' : 'ext_convert',\n        'cfg' : 'cfg_convert',\n    }\n\n    # We might want to use a different one, e.g. importlib\n    importer = __import__\n\n    def __init__(self, config):\n        self.config = ConvertingDict(config)\n        self.config.configurator = self\n\n    def resolve(self, s):\n        \"\"\"\n        Resolve strings to objects using standard import and attribute\n        syntax.\n        \"\"\"\n        name = s.split('.')\n        used = name.pop(0)\n        try:\n            found = self.importer(used)\n            for frag in name:\n                used += '.' + frag\n                try:\n                    found = getattr(found, frag)\n                except AttributeError:\n                    self.importer(used)\n                    found = getattr(found, frag)\n            return found\n        except ImportError:\n            e, tb = sys.exc_info()[1:]\n            v = ValueError('Cannot resolve %r: %s' % (s, e))\n            v.__cause__, v.__traceback__ = e, tb\n            raise v\n\n    def ext_convert(self, value):\n        \"\"\"Default converter for the ext:// protocol.\"\"\"\n        return self.resolve(value)\n\n    def cfg_convert(self, value):\n        \"\"\"Default converter for the cfg:// protocol.\"\"\"\n        rest = value\n        m = self.WORD_PATTERN.match(rest)\n        if m is None:\n            raise ValueError(\"Unable to convert %r\" % value)\n        else:\n            rest = rest[m.end():]\n            d = self.config[m.groups()[0]]\n            #print d, rest\n            while rest:\n                m = self.DOT_PATTERN.match(rest)\n                if m:\n                    d = d[m.groups()[0]]\n                else:\n                    m = self.INDEX_PATTERN.match(rest)\n                    if m:\n                        idx = m.groups()[0]\n                        if not self.DIGIT_PATTERN.match(idx):\n                            d = d[idx]\n                        else:\n                            try:\n                                n = int(idx) # try as number first (most likely)\n                                d = d[n]\n                            except TypeError:\n                                d = d[idx]\n                if m:\n                    rest = rest[m.end():]\n                else:\n                    raise ValueError('Unable to convert '\n                                     '%r at %r' % (value, rest))\n        #rest should be empty\n        return d\n\n    def convert(self, value):\n        \"\"\"\n        Convert values to an appropriate type. dicts, lists and tuples are\n        replaced by their converting alternatives. Strings are checked to\n        see if they have a conversion format and are converted if they do.\n        \"\"\"\n        if not isinstance(value, ConvertingDict) and isinstance(value, dict):\n            value = ConvertingDict(value)\n            value.configurator = self\n        elif not isinstance(value, ConvertingList) and isinstance(value, list):\n            value = ConvertingList(value)\n            value.configurator = self\n        elif not isinstance(value, ConvertingTuple) and\\\n                 isinstance(value, tuple):\n            value = ConvertingTuple(value)\n            value.configurator = self\n        elif isinstance(value, string_types): # str for py3k\n            m = self.CONVERT_PATTERN.match(value)\n            if m:\n                d = m.groupdict()\n                prefix = d['prefix']\n                converter = self.value_converters.get(prefix, None)\n                if converter:\n                    suffix = d['suffix']\n                    converter = getattr(self, converter)\n                    value = converter(suffix)\n        return value\n\n    def configure_custom(self, config):\n        \"\"\"Configure an object with a user-supplied factory.\"\"\"\n        c = config.pop('()')\n        if not hasattr(c, '__call__') and type(c) != type:\n            c = self.resolve(c)\n        props = config.pop('.', None)\n        # Check for valid identifiers\n        kwargs = dict([(k, config[k]) for k in config if valid_ident(k)])\n        result = c(**kwargs)\n        if props:\n            for name, value in props.items():\n                setattr(result, name, value)\n        return result\n\n    def as_tuple(self, value):\n        \"\"\"Utility function which converts lists to tuples.\"\"\"\n        if isinstance(value, list):\n            value = tuple(value)\n        return value\n\nclass DictConfigurator(BaseConfigurator):\n    \"\"\"\n    Configure logging using a dictionary-like object to describe the\n    configuration.\n    \"\"\"\n\n    def configure(self):\n        \"\"\"Do the configuration.\"\"\"\n\n        config = self.config\n        if 'version' not in config:\n            raise ValueError(\"dictionary doesn't specify a version\")\n        if config['version'] != 1:\n            raise ValueError(\"Unsupported version: %s\" % config['version'])\n        incremental = config.pop('incremental', False)\n        EMPTY_DICT = {}\n        logging._acquireLock()\n        try:\n            if incremental:\n                handlers = config.get('handlers', EMPTY_DICT)\n                # incremental handler config only if handler name\n                # ties in to logging._handlers (Python 2.7)\n                if sys.version_info[:2] == (2, 7):\n                    for name in handlers:\n                        if name not in logging._handlers:\n                            raise ValueError('No handler found with '\n                                             'name %r'  % name)\n                        else:\n                            try:\n                                handler = logging._handlers[name]\n                                handler_config = handlers[name]\n                                level = handler_config.get('level', None)\n                                if level:\n                                    handler.setLevel(_checkLevel(level))\n                            except Exception as e:\n                                raise ValueError('Unable to configure handler '\n                                                 '%r: %s' % (name, e))\n                loggers = config.get('loggers', EMPTY_DICT)\n                for name in loggers:\n                    try:\n                        self.configure_logger(name, loggers[name], True)\n                    except Exception as e:\n                        raise ValueError('Unable to configure logger '\n                                         '%r: %s' % (name, e))\n                root = config.get('root', None)\n                if root:\n                    try:\n                        self.configure_root(root, True)\n                    except Exception as e:\n                        raise ValueError('Unable to configure root '\n                                         'logger: %s' % e)\n            else:\n                disable_existing = config.pop('disable_existing_loggers', True)\n\n                logging._handlers.clear()\n                del logging._handlerList[:]\n\n                # Do formatters first - they don't refer to anything else\n                formatters = config.get('formatters', EMPTY_DICT)\n                for name in formatters:\n                    try:\n                        formatters[name] = self.configure_formatter(\n                                                            formatters[name])\n                    except Exception as e:\n                        raise ValueError('Unable to configure '\n                                         'formatter %r: %s' % (name, e))\n                # Next, do filters - they don't refer to anything else, either\n                filters = config.get('filters', EMPTY_DICT)\n                for name in filters:\n                    try:\n                        filters[name] = self.configure_filter(filters[name])\n                    except Exception as e:\n                        raise ValueError('Unable to configure '\n                                         'filter %r: %s' % (name, e))\n\n                # Next, do handlers - they refer to formatters and filters\n                # As handlers can refer to other handlers, sort the keys\n                # to allow a deterministic order of configuration\n                handlers = config.get('handlers', EMPTY_DICT)\n                for name in sorted(handlers):\n                    try:\n                        handler = self.configure_handler(handlers[name])\n                        handler.name = name\n                        handlers[name] = handler\n                    except Exception as e:\n                        raise ValueError('Unable to configure handler '\n                                         '%r: %s' % (name, e))\n                # Next, do loggers - they refer to handlers and filters\n\n                #we don't want to lose the existing loggers,\n                #since other threads may have pointers to them.\n                #existing is set to contain all existing loggers,\n                #and as we go through the new configuration we\n                #remove any which are configured. At the end,\n                #what's left in existing is the set of loggers\n                #which were in the previous configuration but\n                #which are not in the new configuration.\n                root = logging.root\n                existing = root.manager.loggerDict.keys()\n                #The list needs to be sorted so that we can\n                #avoid disabling child loggers of explicitly\n                #named loggers. With a sorted list it is easier\n                #to find the child loggers.\n                existing.sort()\n                #We'll keep the list of existing loggers\n                #which are children of named loggers here...\n                child_loggers = []\n                #now set up the new ones...\n                loggers = config.get('loggers', EMPTY_DICT)\n                for name in loggers:\n                    if name in existing:\n                        i = existing.index(name)\n                        prefixed = name + \".\"\n                        pflen = len(prefixed)\n                        num_existing = len(existing)\n                        i = i + 1 # look at the entry after name\n                        while (i < num_existing) and\\\n                              (existing[i][:pflen] == prefixed):\n                            child_loggers.append(existing[i])\n                            i = i + 1\n                        existing.remove(name)\n                    try:\n                        self.configure_logger(name, loggers[name])\n                    except Exception as e:\n                        raise ValueError('Unable to configure logger '\n                                         '%r: %s' % (name, e))\n\n                #Disable any old loggers. There's no point deleting\n                #them as other threads may continue to hold references\n                #and by disabling them, you stop them doing any logging.\n                #However, don't disable children of named loggers, as that's\n                #probably not what was intended by the user.\n                for log in existing:\n                    logger = root.manager.loggerDict[log]\n                    if log in child_loggers:\n                        logger.level = logging.NOTSET\n                        logger.handlers = []\n                        logger.propagate = True\n                    elif disable_existing:\n                        logger.disabled = True\n\n                # And finally, do the root logger\n                root = config.get('root', None)\n                if root:\n                    try:\n                        self.configure_root(root)\n                    except Exception as e:\n                        raise ValueError('Unable to configure root '\n                                         'logger: %s' % e)\n        finally:\n            logging._releaseLock()\n\n    def configure_formatter(self, config):\n        \"\"\"Configure a formatter from a dictionary.\"\"\"\n        if '()' in config:\n            factory = config['()'] # for use in exception handler\n            try:\n                result = self.configure_custom(config)\n            except TypeError as te:\n                if \"'format'\" not in str(te):\n                    raise\n                #Name of parameter changed from fmt to format.\n                #Retry with old name.\n                #This is so that code can be used with older Python versions\n                #(e.g. by Django)\n                config['fmt'] = config.pop('format')\n                config['()'] = factory\n                result = self.configure_custom(config)\n        else:\n            fmt = config.get('format', None)\n            dfmt = config.get('datefmt', None)\n            result = logging.Formatter(fmt, dfmt)\n        return result\n\n    def configure_filter(self, config):\n        \"\"\"Configure a filter from a dictionary.\"\"\"\n        if '()' in config:\n            result = self.configure_custom(config)\n        else:\n            name = config.get('name', '')\n            result = logging.Filter(name)\n        return result\n\n    def add_filters(self, filterer, filters):\n        \"\"\"Add filters to a filterer from a list of names.\"\"\"\n        for f in filters:\n            try:\n                filterer.addFilter(self.config['filters'][f])\n            except Exception as e:\n                raise ValueError('Unable to add filter %r: %s' % (f, e))\n\n    def configure_handler(self, config):\n        \"\"\"Configure a handler from a dictionary.\"\"\"\n        formatter = config.pop('formatter', None)\n        if formatter:\n            try:\n                formatter = self.config['formatters'][formatter]\n            except Exception as e:\n                raise ValueError('Unable to set formatter '\n                                 '%r: %s' % (formatter, e))\n        level = config.pop('level', None)\n        filters = config.pop('filters', None)\n        if '()' in config:\n            c = config.pop('()')\n            if not hasattr(c, '__call__') and type(c) != type:\n                c = self.resolve(c)\n            factory = c\n        else:\n            klass = self.resolve(config.pop('class'))\n            #Special case for handler which refers to another handler\n            if issubclass(klass, logging.handlers.MemoryHandler) and\\\n                'target' in config:\n                try:\n                    config['target'] = self.config['handlers'][config['target']]\n                except Exception as e:\n                    raise ValueError('Unable to set target handler '\n                                     '%r: %s' % (config['target'], e))\n            elif issubclass(klass, logging.handlers.SMTPHandler) and\\\n                'mailhost' in config:\n                config['mailhost'] = self.as_tuple(config['mailhost'])\n            elif issubclass(klass, logging.handlers.SysLogHandler) and\\\n                'address' in config:\n                config['address'] = self.as_tuple(config['address'])\n            factory = klass\n        kwargs = dict([(str(k), config[k]) for k in config if valid_ident(k)])\n        try:\n            result = factory(**kwargs)\n        except TypeError as te:\n            if \"'stream'\" not in str(te):\n                raise\n            #The argument name changed from strm to stream\n            #Retry with old name.\n            #This is so that code can be used with older Python versions\n            #(e.g. by Django)\n            kwargs['strm'] = kwargs.pop('stream')\n            result = factory(**kwargs)\n        if formatter:\n            result.setFormatter(formatter)\n        if level is not None:\n            result.setLevel(_checkLevel(level))\n        if filters:\n            self.add_filters(result, filters)\n        return result\n\n    def add_handlers(self, logger, handlers):\n        \"\"\"Add handlers to a logger from a list of names.\"\"\"\n        for h in handlers:\n            try:\n                logger.addHandler(self.config['handlers'][h])\n            except Exception as e:\n                raise ValueError('Unable to add handler %r: %s' % (h, e))\n\n    def common_logger_config(self, logger, config, incremental=False):\n        \"\"\"\n        Perform configuration which is common to root and non-root loggers.\n        \"\"\"\n        level = config.get('level', None)\n        if level is not None:\n            logger.setLevel(_checkLevel(level))\n        if not incremental:\n            #Remove any existing handlers\n            for h in logger.handlers[:]:\n                logger.removeHandler(h)\n            handlers = config.get('handlers', None)\n            if handlers:\n                self.add_handlers(logger, handlers)\n            filters = config.get('filters', None)\n            if filters:\n                self.add_filters(logger, filters)\n\n    def configure_logger(self, name, config, incremental=False):\n        \"\"\"Configure a non-root logger from a dictionary.\"\"\"\n        logger = logging.getLogger(name)\n        self.common_logger_config(logger, config, incremental)\n        propagate = config.get('propagate', None)\n        if propagate is not None:\n            logger.propagate = propagate\n\n    def configure_root(self, config, incremental=False):\n        \"\"\"Configure a root logger from a dictionary.\"\"\"\n        root = logging.getLogger()\n        self.common_logger_config(root, config, incremental)\n\ndictConfigClass = DictConfigurator\n\ndef dictConfig(config):\n    \"\"\"Configure logging using a dictionary.\"\"\"\n    dictConfigClass(config).configure()\n"
                        }
                    ]
                }, 
                {
                    "type": "text", 
                    "name": "connections.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom contextlib import contextmanager\n\nfrom redis import StrictRedis\n\nfrom .compat.connections import patch_connection\nfrom .local import LocalStack, release_local\n\n\nclass NoRedisConnectionException(Exception):\n    pass\n\n\n@contextmanager\ndef Connection(connection=None):\n    if connection is None:\n        connection = StrictRedis()\n    push_connection(connection)\n    try:\n        yield\n    finally:\n        popped = pop_connection()\n        assert popped == connection, \\\n            'Unexpected Redis connection was popped off the stack. ' \\\n            'Check your Redis connection setup.'\n\n\ndef push_connection(redis):\n    \"\"\"Pushes the given connection on the stack.\"\"\"\n    _connection_stack.push(patch_connection(redis))\n\n\ndef pop_connection():\n    \"\"\"Pops the topmost connection from the stack.\"\"\"\n    return _connection_stack.pop()\n\n\ndef use_connection(redis=None):\n    \"\"\"Clears the stack and uses the given connection.  Protects against mixed\n    use of use_connection() and stacked connection contexts.\n    \"\"\"\n    assert len(_connection_stack) <= 1, \\\n        'You should not mix Connection contexts with use_connection().'\n    release_local(_connection_stack)\n\n    if redis is None:\n        redis = StrictRedis()\n    push_connection(redis)\n\n\ndef get_current_connection():\n    \"\"\"Returns the current Redis connection (i.e. the topmost on the\n    connection stack).\n    \"\"\"\n    return _connection_stack.top\n\n\ndef resolve_connection(connection=None):\n    \"\"\"Convenience function to resolve the given or the current connection.\n    Raises an exception if it cannot resolve a connection now.\n    \"\"\"\n    if connection is not None:\n        return patch_connection(connection)\n\n    connection = get_current_connection()\n    if connection is None:\n        raise NoRedisConnectionException('Could not resolve a Redis connection.')\n    return connection\n\n\n_connection_stack = LocalStack()\n\n__all__ = ['Connection', 'get_current_connection', 'push_connection',\n           'pop_connection', 'use_connection']\n"
                }, 
                {
                    "type": "dir", 
                    "name": "contrib", 
                    "contents": [
                        {
                            "type": "text", 
                            "name": "__init__.py", 
                            "contents": ""
                        }, 
                        {
                            "type": "text", 
                            "name": "legacy.py", 
                            "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\n\nimport logging\nfrom rq import get_current_connection\nfrom rq import Worker\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef cleanup_ghosts():\n    \"\"\"\n    RQ versions < 0.3.6 suffered from a race condition where workers, when\n    abruptly terminated, did not have a chance to clean up their worker\n    registration, leading to reports of ghosted workers in `rqinfo`.  Since\n    0.3.6, new worker registrations automatically expire, and the worker will\n    make sure to refresh the registrations as long as it's alive.\n\n    This function will clean up any of such legacy ghosted workers.\n    \"\"\"\n    conn = get_current_connection()\n    for worker in Worker.all():\n        if conn._ttl(worker.key) == -1:\n            ttl = worker.default_worker_ttl\n            conn.expire(worker.key, ttl)\n            logger.info('Marked ghosted worker {0} to expire in {1} seconds.'.format(worker.name, ttl))\n"
                        }, 
                        {
                            "type": "text", 
                            "name": "sentry.py", 
                            "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nimport warnings\n\n\ndef register_sentry(client, worker):\n    \"\"\"Given a Raven client and an RQ worker, registers exception handlers\n    with the worker so exceptions are logged to Sentry.\n    \"\"\"\n    def uses_supported_transport(url):\n        supported_transports = set(['sync+', 'requests+'])\n        return any(url.startswith(prefix) for prefix in supported_transports)\n\n    if not any(uses_supported_transport(s) for s in client.servers):\n        msg = ('Sentry error delivery is known to be unreliable when not '\n               'delivered synchronously from RQ workers.  You are encouraged '\n               'to change your DSN to use the sync+ or requests+ transport '\n               'prefix.')\n        warnings.warn(msg, UserWarning, stacklevel=2)\n\n    def send_to_sentry(job, *exc_info):\n        client.captureException(\n            exc_info=exc_info,\n            extra={\n                'job_id': job.id,\n                'func': job.func_name,\n                'args': job.args,\n                'kwargs': job.kwargs,\n                'description': job.description,\n            })\n\n    worker.push_exc_handler(send_to_sentry)\n"
                        }
                    ]
                }, 
                {
                    "type": "text", 
                    "name": "decorators.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom functools import wraps\n\nfrom rq.compat import string_types\n\nfrom .queue import Queue\nfrom .worker import DEFAULT_RESULT_TTL\n\n\nclass job(object):\n    def __init__(self, queue, connection=None, timeout=None,\n                 result_ttl=DEFAULT_RESULT_TTL):\n        \"\"\"A decorator that adds a ``delay`` method to the decorated function,\n        which in turn creates a RQ job when called. Accepts a required\n        ``queue`` argument that can be either a ``Queue`` instance or a string\n        denoting the queue name.  For example:\n\n            @job(queue='default')\n            def simple_add(x, y):\n                return x + y\n\n            simple_add.delay(1, 2) # Puts simple_add function into queue\n        \"\"\"\n        self.queue = queue\n        self.connection = connection\n        self.timeout = timeout\n        self.result_ttl = result_ttl\n\n    def __call__(self, f):\n        @wraps(f)\n        def delay(*args, **kwargs):\n            if isinstance(self.queue, string_types):\n                queue = Queue(name=self.queue, connection=self.connection)\n            else:\n                queue = self.queue\n            if 'depends_on' in kwargs:\n                depends_on = kwargs.pop('depends_on')\n            else:\n                depends_on = None\n            return queue.enqueue_call(f, args=args, kwargs=kwargs,\n                                      timeout=self.timeout, result_ttl=self.result_ttl, depends_on=depends_on)\n        f.delay = delay\n        return f\n"
                }, 
                {
                    "type": "text", 
                    "name": "dummy.py", 
                    "contents": "# -*- coding: utf-8 -*-\n\"\"\"\nSome dummy tasks that are well-suited for generating load for testing purposes.\n\"\"\"\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport random\nimport time\n\n\ndef do_nothing():\n    pass\n\n\ndef sleep(secs):\n    time.sleep(secs)\n\n\ndef endless_loop():\n    while True:\n        time.sleep(1)\n\n\ndef div_by_zero():\n    1 / 0\n\n\ndef fib(n):\n    if n <= 1:\n        return 1\n    else:\n        return fib(n - 2) + fib(n - 1)\n\n\ndef random_failure():\n    if random.choice([True, False]):\n        class RandomError(Exception):\n            pass\n        raise RandomError('Ouch!')\n    return 'OK'\n"
                }, 
                {
                    "type": "text", 
                    "name": "exceptions.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\n\nclass NoSuchJobError(Exception):\n    pass\n\n\nclass InvalidJobOperationError(Exception):\n    pass\n\n\nclass NoQueueError(Exception):\n    pass\n\n\nclass UnpickleError(Exception):\n    def __init__(self, message, raw_data, inner_exception=None):\n        super(UnpickleError, self).__init__(message, inner_exception)\n        self.raw_data = raw_data\n\n\nclass DequeueTimeout(Exception):\n    pass\n"
                }, 
                {
                    "type": "text", 
                    "name": "job.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport inspect\nfrom uuid import uuid4\n\nfrom rq.compat import as_text, decode_redis_hash, string_types, text_type\n\nfrom .connections import resolve_connection\nfrom .exceptions import NoSuchJobError, UnpickleError\nfrom .local import LocalStack\nfrom .utils import import_attribute, utcformat, utcnow, utcparse\n\ntry:\n    from cPickle import loads, dumps, UnpicklingError\nexcept ImportError:  # noqa\n    from pickle import loads, dumps, UnpicklingError  # noqa\n\n\ndef enum(name, *sequential, **named):\n    values = dict(zip(sequential, range(len(sequential))), **named)\n\n    # NOTE: Yes, we *really* want to cast using str() here.\n    # On Python 2 type() requires a byte string (which is str() on Python 2).\n    # On Python 3 it does not matter, so we'll use str(), which acts as\n    # a no-op.\n    return type(str(name), (), values)\n\nStatus = enum('Status',\n              QUEUED='queued', FINISHED='finished', FAILED='failed',\n              STARTED='started')\n\n# Sentinel value to mark that some of our lazily evaluated properties have not\n# yet been evaluated.\nUNEVALUATED = object()\n\n\ndef unpickle(pickled_string):\n    \"\"\"Unpickles a string, but raises a unified UnpickleError in case anything\n    fails.\n\n    This is a helper method to not have to deal with the fact that `loads()`\n    potentially raises many types of exceptions (e.g. AttributeError,\n    IndexError, TypeError, KeyError, etc.)\n    \"\"\"\n    try:\n        obj = loads(pickled_string)\n    except (Exception, UnpicklingError) as e:\n        raise UnpickleError('Could not unpickle.', pickled_string, e)\n    return obj\n\n\ndef cancel_job(job_id, connection=None):\n    \"\"\"Cancels the job with the given job ID, preventing execution.  Discards\n    any job info (i.e. it can't be requeued later).\n    \"\"\"\n    Job(job_id, connection=connection).cancel()\n\n\ndef requeue_job(job_id, connection=None):\n    \"\"\"Requeues the job with the given job ID.  The job ID should refer to\n    a failed job (i.e. it should be on the failed queue).  If no such (failed)\n    job exists, a NoSuchJobError is raised.\n    \"\"\"\n    from .queue import get_failed_queue\n    fq = get_failed_queue(connection=connection)\n    fq.requeue(job_id)\n\n\ndef get_current_job(connection=None):\n    \"\"\"Returns the Job instance that is currently being executed.  If this\n    function is invoked from outside a job context, None is returned.\n    \"\"\"\n    job_id = _job_stack.top\n    if job_id is None:\n        return None\n    return Job.fetch(job_id, connection=connection)\n\n\nclass Job(object):\n    \"\"\"A Job is just a convenient datastructure to pass around job (meta) data.\n    \"\"\"\n\n    # Job construction\n    @classmethod\n    def create(cls, func, args=None, kwargs=None, connection=None,\n               result_ttl=None, status=None, description=None, depends_on=None, timeout=None):\n        \"\"\"Creates a new Job instance for the given function, arguments, and\n        keyword arguments.\n        \"\"\"\n        if args is None:\n            args = ()\n        if kwargs is None:\n            kwargs = {}\n\n        if not isinstance(args, (tuple, list)):\n            raise TypeError('{0!r} is not a valid args list.'.format(args))\n        if not isinstance(kwargs, dict):\n            raise TypeError('{0!r} is not a valid kwargs dict.'.format(kwargs))\n\n        job = cls(connection=connection)\n\n        # Set the core job tuple properties\n        job._instance = None\n        if inspect.ismethod(func):\n            job._instance = func.__self__\n            job._func_name = func.__name__\n        elif inspect.isfunction(func) or inspect.isbuiltin(func):\n            job._func_name = '%s.%s' % (func.__module__, func.__name__)\n        elif isinstance(func, string_types):\n            job._func_name = as_text(func)\n        else:\n            raise TypeError('Expected a function/method/string, but got: {}'.format(func))\n        job._args = args\n        job._kwargs = kwargs\n\n        # Extra meta data\n        job.description = description or job.get_call_string()\n        job.result_ttl = result_ttl\n        job.timeout = timeout\n        job._status = status\n\n        # dependency could be job instance or id\n        if depends_on is not None:\n            job._dependency_id = depends_on.id if isinstance(depends_on, Job) else depends_on\n        return job\n\n    def get_status(self):\n        self._status = as_text(self.connection.hget(self.key, 'status'))\n        return self._status\n\n    def _get_status(self):\n        raise DeprecationWarning(\n            \"job.status is deprecated. Use job.get_status() instead\"\n        )\n        return self.get_status()\n\n    def set_status(self, status):\n        self._status = status\n        self.connection.hset(self.key, 'status', self._status)\n\n    def _set_status(self, status):\n        raise DeprecationWarning(\n            \"job.status is deprecated. Use job.set_status() instead\"\n        )\n        self.set_status(status)\n\n    status = property(_get_status, _set_status)\n\n    @property\n    def is_finished(self):\n        return self.get_status() == Status.FINISHED\n\n    @property\n    def is_queued(self):\n        return self.get_status() == Status.QUEUED\n\n    @property\n    def is_failed(self):\n        return self.get_status() == Status.FAILED\n\n    @property\n    def is_started(self):\n        return self.get_status() == Status.STARTED\n\n    @property\n    def dependency(self):\n        \"\"\"Returns a job's dependency. To avoid repeated Redis fetches, we cache\n        job.dependency as job._dependency.\n        \"\"\"\n        if self._dependency_id is None:\n            return None\n        if hasattr(self, '_dependency'):\n            return self._dependency\n        job = Job.fetch(self._dependency_id, connection=self.connection)\n        job.refresh()\n        self._dependency = job\n        return job\n\n    @property\n    def func(self):\n        func_name = self.func_name\n        if func_name is None:\n            return None\n\n        if self.instance:\n            return getattr(self.instance, func_name)\n\n        return import_attribute(self.func_name)\n\n    def _unpickle_data(self):\n        self._func_name, self._instance, self._args, self._kwargs = unpickle(self.data)\n\n    @property\n    def data(self):\n        if self._data is UNEVALUATED:\n            if self._func_name is UNEVALUATED:\n                raise ValueError('Cannot build the job data.')\n\n            if self._instance is UNEVALUATED:\n                self._instance = None\n\n            if self._args is UNEVALUATED:\n                self._args = ()\n\n            if self._kwargs is UNEVALUATED:\n                self._kwargs = {}\n\n            job_tuple = self._func_name, self._instance, self._args, self._kwargs\n            self._data = dumps(job_tuple)\n        return self._data\n\n    @data.setter\n    def data(self, value):\n        self._data = value\n        self._func_name = UNEVALUATED\n        self._instance = UNEVALUATED\n        self._args = UNEVALUATED\n        self._kwargs = UNEVALUATED\n\n    @property\n    def func_name(self):\n        if self._func_name is UNEVALUATED:\n            self._unpickle_data()\n        return self._func_name\n\n    @func_name.setter\n    def func_name(self, value):\n        self._func_name = value\n        self._data = UNEVALUATED\n\n    @property\n    def instance(self):\n        if self._instance is UNEVALUATED:\n            self._unpickle_data()\n        return self._instance\n\n    @instance.setter\n    def instance(self, value):\n        self._instance = value\n        self._data = UNEVALUATED\n\n    @property\n    def args(self):\n        if self._args is UNEVALUATED:\n            self._unpickle_data()\n        return self._args\n\n    @args.setter\n    def args(self, value):\n        self._args = value\n        self._data = UNEVALUATED\n\n    @property\n    def kwargs(self):\n        if self._kwargs is UNEVALUATED:\n            self._unpickle_data()\n        return self._kwargs\n\n    @kwargs.setter\n    def kwargs(self, value):\n        self._kwargs = value\n        self._data = UNEVALUATED\n\n    @classmethod\n    def exists(cls, job_id, connection=None):\n        \"\"\"Returns whether a job hash exists for the given job ID.\"\"\"\n        conn = resolve_connection(connection)\n        return conn.exists(cls.key_for(job_id))\n\n    @classmethod\n    def fetch(cls, id, connection=None):\n        \"\"\"Fetches a persisted job from its corresponding Redis key and\n        instantiates it.\n        \"\"\"\n        job = cls(id, connection=connection)\n        job.refresh()\n        return job\n\n    def __init__(self, id=None, connection=None):\n        self.connection = resolve_connection(connection)\n        self._id = id\n        self.created_at = utcnow()\n        self._data = UNEVALUATED\n        self._func_name = UNEVALUATED\n        self._instance = UNEVALUATED\n        self._args = UNEVALUATED\n        self._kwargs = UNEVALUATED\n        self.description = None\n        self.origin = None\n        self.enqueued_at = None\n        self.ended_at = None\n        self._result = None\n        self.exc_info = None\n        self.timeout = None\n        self.result_ttl = None\n        self._status = None\n        self._dependency_id = None\n        self.meta = {}\n\n    def __repr__(self):  # noqa\n        return 'Job(%r, enqueued_at=%r)' % (self._id, self.enqueued_at)\n\n    # Data access\n    def get_id(self):  # noqa\n        \"\"\"The job ID for this job instance. Generates an ID lazily the\n        first time the ID is requested.\n        \"\"\"\n        if self._id is None:\n            self._id = text_type(uuid4())\n        return self._id\n\n    def set_id(self, value):\n        \"\"\"Sets a job ID for the given job.\"\"\"\n        self._id = value\n\n    id = property(get_id, set_id)\n\n    @classmethod\n    def key_for(cls, job_id):\n        \"\"\"The Redis key that is used to store job hash under.\"\"\"\n        return b'rq:job:' + job_id.encode('utf-8')\n\n    @classmethod\n    def dependents_key_for(cls, job_id):\n        \"\"\"The Redis key that is used to store job hash under.\"\"\"\n        return 'rq:job:%s:dependents' % (job_id,)\n\n    @property\n    def key(self):\n        \"\"\"The Redis key that is used to store job hash under.\"\"\"\n        return self.key_for(self.id)\n\n    @property\n    def dependents_key(self):\n        \"\"\"The Redis key that is used to store job hash under.\"\"\"\n        return self.dependents_key_for(self.id)\n\n    @property\n    def result(self):\n        \"\"\"Returns the return value of the job.\n\n        Initially, right after enqueueing a job, the return value will be\n        None.  But when the job has been executed, and had a return value or\n        exception, this will return that value or exception.\n\n        Note that, when the job has no return value (i.e. returns None), the\n        ReadOnlyJob object is useless, as the result won't be written back to\n        Redis.\n\n        Also note that you cannot draw the conclusion that a job has _not_\n        been executed when its return value is None, since return values\n        written back to Redis will expire after a given amount of time (500\n        seconds by default).\n        \"\"\"\n        if self._result is None:\n            rv = self.connection.hget(self.key, 'result')\n            if rv is not None:\n                # cache the result\n                self._result = loads(rv)\n        return self._result\n\n    \"\"\"Backwards-compatibility accessor property `return_value`.\"\"\"\n    return_value = result\n\n    # Persistence\n    def refresh(self):  # noqa\n        \"\"\"Overwrite the current instance's properties with the values in the\n        corresponding Redis key.\n\n        Will raise a NoSuchJobError if no corresponding Redis key exists.\n        \"\"\"\n        key = self.key\n        obj = decode_redis_hash(self.connection.hgetall(key))\n        if len(obj) == 0:\n            raise NoSuchJobError('No such job: %s' % (key,))\n\n        def to_date(date_str):\n            if date_str is None:\n                return\n            else:\n                return utcparse(as_text(date_str))\n\n        try:\n            self.data = obj['data']\n        except KeyError:\n            raise NoSuchJobError('Unexpected job format: {0}'.format(obj))\n\n        self.created_at = to_date(as_text(obj.get('created_at')))\n        self.origin = as_text(obj.get('origin'))\n        self.description = as_text(obj.get('description'))\n        self.enqueued_at = to_date(as_text(obj.get('enqueued_at')))\n        self.ended_at = to_date(as_text(obj.get('ended_at')))\n        self._result = unpickle(obj.get('result')) if obj.get('result') else None  # noqa\n        self.exc_info = obj.get('exc_info')\n        self.timeout = int(obj.get('timeout')) if obj.get('timeout') else None\n        self.result_ttl = int(obj.get('result_ttl')) if obj.get('result_ttl') else None  # noqa\n        self._status = as_text(obj.get('status') if obj.get('status') else None)\n        self._dependency_id = as_text(obj.get('dependency_id', None))\n        self.meta = unpickle(obj.get('meta')) if obj.get('meta') else {}\n\n    def dump(self):\n        \"\"\"Returns a serialization of the current job instance\"\"\"\n        obj = {}\n        obj['created_at'] = utcformat(self.created_at or utcnow())\n        obj['data'] = self.data\n\n        if self.origin is not None:\n            obj['origin'] = self.origin\n        if self.description is not None:\n            obj['description'] = self.description\n        if self.enqueued_at is not None:\n            obj['enqueued_at'] = utcformat(self.enqueued_at)\n        if self.ended_at is not None:\n            obj['ended_at'] = utcformat(self.ended_at)\n        if self._result is not None:\n            obj['result'] = dumps(self._result)\n        if self.exc_info is not None:\n            obj['exc_info'] = self.exc_info\n        if self.timeout is not None:\n            obj['timeout'] = self.timeout\n        if self.result_ttl is not None:\n            obj['result_ttl'] = self.result_ttl\n        if self._status is not None:\n            obj['status'] = self._status\n        if self._dependency_id is not None:\n            obj['dependency_id'] = self._dependency_id\n        if self.meta:\n            obj['meta'] = dumps(self.meta)\n\n        return obj\n\n    def save(self, pipeline=None):\n        \"\"\"Persists the current job instance to its corresponding Redis key.\"\"\"\n        key = self.key\n        connection = pipeline if pipeline is not None else self.connection\n\n        connection.hmset(key, self.dump())\n\n    def cancel(self):\n        \"\"\"Cancels the given job, which will prevent the job from ever being\n        ran (or inspected).\n\n        This method merely exists as a high-level API call to cancel jobs\n        without worrying about the internals required to implement job\n        cancellation.  Technically, this call is (currently) the same as just\n        deleting the job hash.\n        \"\"\"\n        pipeline = self.connection._pipeline()\n        self.delete(pipeline=pipeline)\n        pipeline.delete(self.dependents_key)\n        pipeline.execute()\n\n    def delete(self, pipeline=None):\n        \"\"\"Deletes the job hash from Redis.\"\"\"\n        connection = pipeline if pipeline is not None else self.connection\n        connection.delete(self.key)\n\n    # Job execution\n    def perform(self):  # noqa\n        \"\"\"Invokes the job function with the job arguments.\"\"\"\n        _job_stack.push(self.id)\n        try:\n            self.set_status(Status.STARTED)\n            self._result = self.func(*self.args, **self.kwargs)\n            self.set_status(Status.FINISHED)\n            self.ended_at = utcnow()\n        finally:\n            assert self.id == _job_stack.pop()\n\n        return self._result\n\n    def get_ttl(self, default_ttl=None):\n        \"\"\"Returns ttl for a job that determines how long a job and its result\n        will be persisted. In the future, this method will also be responsible\n        for determining ttl for repeated jobs.\n        \"\"\"\n        return default_ttl if self.result_ttl is None else self.result_ttl\n\n    # Representation\n    def get_call_string(self):  # noqa\n        \"\"\"Returns a string representation of the call, formatted as a regular\n        Python function invocation statement.\n        \"\"\"\n        if self.func_name is None:\n            return None\n\n        arg_list = [repr(arg) for arg in self.args]\n        arg_list += ['%s=%r' % (k, v) for k, v in self.kwargs.items()]\n        args = ', '.join(arg_list)\n        return '%s(%s)' % (self.func_name, args)\n\n    def cleanup(self, ttl=None, pipeline=None):\n        \"\"\"Prepare job for eventual deletion (if needed). This method is usually\n        called after successful execution. How long we persist the job and its\n        result depends on the value of result_ttl:\n        - If result_ttl is 0, cleanup the job immediately.\n        - If it's a positive number, set the job to expire in X seconds.\n        - If result_ttl is negative, don't set an expiry to it (persist\n          forever)\n        \"\"\"\n        if ttl == 0:\n            self.cancel()\n        elif ttl > 0:\n            connection = pipeline if pipeline is not None else self.connection\n            connection.expire(self.key, ttl)\n\n    def register_dependency(self):\n        \"\"\"Jobs may have dependencies. Jobs are enqueued only if the job they\n        depend on is successfully performed. We record this relation as\n        a reverse dependency (a Redis set), with a key that looks something\n        like:\n\n            rq:job:job_id:dependents = {'job_id_1', 'job_id_2'}\n\n        This method adds the current job in its dependency's dependents set.\n        \"\"\"\n        # TODO: This can probably be pipelined\n        self.connection.sadd(Job.dependents_key_for(self._dependency_id), self.id)\n\n    def __str__(self):\n        return '<Job %s: %s>' % (self.id, self.description)\n\n    # Job equality\n    def __eq__(self, other):  # noqa\n        return self.id == other.id\n\n    def __hash__(self):\n        return hash(self.id)\n\n_job_stack = LocalStack()\n"
                }, 
                {
                    "type": "text", 
                    "name": "local.py", 
                    "contents": "# -*- coding: utf-8 -*-\n\"\"\"\n    werkzeug.local\n    ~~~~~~~~~~~~~~\n\n    This module implements context-local objects.\n\n    :copyright: (c) 2011 by the Werkzeug Team, see AUTHORS for more details.\n    :license: BSD, see LICENSE for more details.\n\"\"\"\n# Since each thread has its own greenlet we can just use those as identifiers\n# for the context.  If greenlets are not available we fall back to the\n# current thread ident.\ntry:\n    from greenlet import getcurrent as get_ident\nexcept ImportError:  # noqa\n    try:\n        from thread import get_ident  # noqa\n    except ImportError:  # noqa\n        try:\n            from _thread import get_ident  # noqa\n        except ImportError:  # noqa\n            from dummy_thread import get_ident  # noqa\n\n\ndef release_local(local):\n    \"\"\"Releases the contents of the local for the current context.\n    This makes it possible to use locals without a manager.\n\n    Example::\n\n        >>> loc = Local()\n        >>> loc.foo = 42\n        >>> release_local(loc)\n        >>> hasattr(loc, 'foo')\n        False\n\n    With this function one can release :class:`Local` objects as well\n    as :class:`StackLocal` objects.  However it is not possible to\n    release data held by proxies that way, one always has to retain\n    a reference to the underlying local object in order to be able\n    to release it.\n\n    .. versionadded:: 0.6.1\n    \"\"\"\n    local.__release_local__()\n\n\nclass Local(object):\n    __slots__ = ('__storage__', '__ident_func__')\n\n    def __init__(self):\n        object.__setattr__(self, '__storage__', {})\n        object.__setattr__(self, '__ident_func__', get_ident)\n\n    def __iter__(self):\n        return iter(self.__storage__.items())\n\n    def __call__(self, proxy):\n        \"\"\"Create a proxy for a name.\"\"\"\n        return LocalProxy(self, proxy)\n\n    def __release_local__(self):\n        self.__storage__.pop(self.__ident_func__(), None)\n\n    def __getattr__(self, name):\n        try:\n            return self.__storage__[self.__ident_func__()][name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        ident = self.__ident_func__()\n        storage = self.__storage__\n        try:\n            storage[ident][name] = value\n        except KeyError:\n            storage[ident] = {name: value}\n\n    def __delattr__(self, name):\n        try:\n            del self.__storage__[self.__ident_func__()][name]\n        except KeyError:\n            raise AttributeError(name)\n\n\nclass LocalStack(object):\n    \"\"\"This class works similar to a :class:`Local` but keeps a stack\n    of objects instead.  This is best explained with an example::\n\n        >>> ls = LocalStack()\n        >>> ls.push(42)\n        >>> ls.top\n        42\n        >>> ls.push(23)\n        >>> ls.top\n        23\n        >>> ls.pop()\n        23\n        >>> ls.top\n        42\n\n    They can be force released by using a :class:`LocalManager` or with\n    the :func:`release_local` function but the correct way is to pop the\n    item from the stack after using.  When the stack is empty it will\n    no longer be bound to the current context (and as such released).\n\n    By calling the stack without arguments it returns a proxy that resolves to\n    the topmost item on the stack.\n\n    .. versionadded:: 0.6.1\n    \"\"\"\n\n    def __init__(self):\n        self._local = Local()\n\n    def __release_local__(self):\n        self._local.__release_local__()\n\n    def _get__ident_func__(self):\n        return self._local.__ident_func__\n\n    def _set__ident_func__(self, value):  # noqa\n        object.__setattr__(self._local, '__ident_func__', value)\n    __ident_func__ = property(_get__ident_func__, _set__ident_func__)\n    del _get__ident_func__, _set__ident_func__\n\n    def __call__(self):\n        def _lookup():\n            rv = self.top\n            if rv is None:\n                raise RuntimeError('object unbound')\n            return rv\n        return LocalProxy(_lookup)\n\n    def push(self, obj):\n        \"\"\"Pushes a new item to the stack\"\"\"\n        rv = getattr(self._local, 'stack', None)\n        if rv is None:\n            self._local.stack = rv = []\n        rv.append(obj)\n        return rv\n\n    def pop(self):\n        \"\"\"Removes the topmost item from the stack, will return the\n        old value or `None` if the stack was already empty.\n        \"\"\"\n        stack = getattr(self._local, 'stack', None)\n        if stack is None:\n            return None\n        elif len(stack) == 1:\n            release_local(self._local)\n            return stack[-1]\n        else:\n            return stack.pop()\n\n    @property\n    def top(self):\n        \"\"\"The topmost item on the stack.  If the stack is empty,\n        `None` is returned.\n        \"\"\"\n        try:\n            return self._local.stack[-1]\n        except (AttributeError, IndexError):\n            return None\n\n    def __len__(self):\n        stack = getattr(self._local, 'stack', None)\n        if stack is None:\n            return 0\n        return len(stack)\n\n\nclass LocalManager(object):\n    \"\"\"Local objects cannot manage themselves. For that you need a local\n    manager.  You can pass a local manager multiple locals or add them later\n    by appending them to `manager.locals`.  Everytime the manager cleans up\n    it, will clean up all the data left in the locals for this context.\n\n    The `ident_func` parameter can be added to override the default ident\n    function for the wrapped locals.\n\n    .. versionchanged:: 0.6.1\n       Instead of a manager the :func:`release_local` function can be used\n       as well.\n\n    .. versionchanged:: 0.7\n       `ident_func` was added.\n    \"\"\"\n\n    def __init__(self, locals=None, ident_func=None):\n        if locals is None:\n            self.locals = []\n        elif isinstance(locals, Local):\n            self.locals = [locals]\n        else:\n            self.locals = list(locals)\n        if ident_func is not None:\n            self.ident_func = ident_func\n            for local in self.locals:\n                object.__setattr__(local, '__ident_func__', ident_func)\n        else:\n            self.ident_func = get_ident\n\n    def get_ident(self):\n        \"\"\"Return the context identifier the local objects use internally for\n        this context.  You cannot override this method to change the behavior\n        but use it to link other context local objects (such as SQLAlchemy's\n        scoped sessions) to the Werkzeug locals.\n\n        .. versionchanged:: 0.7\n           You can pass a different ident function to the local manager that\n           will then be propagated to all the locals passed to the\n           constructor.\n        \"\"\"\n        return self.ident_func()\n\n    def cleanup(self):\n        \"\"\"Manually clean up the data in the locals for this context.  Call\n        this at the end of the request or use `make_middleware()`.\n        \"\"\"\n        for local in self.locals:\n            release_local(local)\n\n    def __repr__(self):\n        return '<%s storages: %d>' % (\n            self.__class__.__name__,\n            len(self.locals)\n        )\n\n\nclass LocalProxy(object):\n    \"\"\"Acts as a proxy for a werkzeug local.  Forwards all operations to\n    a proxied object.  The only operations not supported for forwarding\n    are right handed operands and any kind of assignment.\n\n    Example usage::\n\n        from werkzeug.local import Local\n        l = Local()\n\n        # these are proxies\n        request = l('request')\n        user = l('user')\n\n\n        from werkzeug.local import LocalStack\n        _response_local = LocalStack()\n\n        # this is a proxy\n        response = _response_local()\n\n    Whenever something is bound to l.user / l.request the proxy objects\n    will forward all operations.  If no object is bound a :exc:`RuntimeError`\n    will be raised.\n\n    To create proxies to :class:`Local` or :class:`LocalStack` objects,\n    call the object as shown above.  If you want to have a proxy to an\n    object looked up by a function, you can (as of Werkzeug 0.6.1) pass\n    a function to the :class:`LocalProxy` constructor::\n\n        session = LocalProxy(lambda: get_current_request().session)\n\n    .. versionchanged:: 0.6.1\n       The class can be instanciated with a callable as well now.\n    \"\"\"\n    __slots__ = ('__local', '__dict__', '__name__')\n\n    def __init__(self, local, name=None):\n        object.__setattr__(self, '_LocalProxy__local', local)\n        object.__setattr__(self, '__name__', name)\n\n    def _get_current_object(self):\n        \"\"\"Return the current object.  This is useful if you want the real\n        object behind the proxy at a time for performance reasons or because\n        you want to pass the object into a different context.\n        \"\"\"\n        if not hasattr(self.__local, '__release_local__'):\n            return self.__local()\n        try:\n            return getattr(self.__local, self.__name__)\n        except AttributeError:\n            raise RuntimeError('no object bound to %s' % self.__name__)\n\n    @property\n    def __dict__(self):\n        try:\n            return self._get_current_object().__dict__\n        except RuntimeError:\n            raise AttributeError('__dict__')\n\n    def __repr__(self):\n        try:\n            obj = self._get_current_object()\n        except RuntimeError:\n            return '<%s unbound>' % self.__class__.__name__\n        return repr(obj)\n\n    def __nonzero__(self):\n        try:\n            return bool(self._get_current_object())\n        except RuntimeError:\n            return False\n\n    def __unicode__(self):\n        try:\n            return unicode(self._get_current_object())\n        except RuntimeError:\n            return repr(self)\n\n    def __dir__(self):\n        try:\n            return dir(self._get_current_object())\n        except RuntimeError:\n            return []\n\n    def __getattr__(self, name):\n        if name == '__members__':\n            return dir(self._get_current_object())\n        return getattr(self._get_current_object(), name)\n\n    def __setitem__(self, key, value):\n        self._get_current_object()[key] = value\n\n    def __delitem__(self, key):\n        del self._get_current_object()[key]\n\n    def __setslice__(self, i, j, seq):\n        self._get_current_object()[i:j] = seq\n\n    def __delslice__(self, i, j):\n        del self._get_current_object()[i:j]\n\n    __setattr__ = lambda x, n, v: setattr(x._get_current_object(), n, v)\n    __delattr__ = lambda x, n: delattr(x._get_current_object(), n)\n    __str__ = lambda x: str(x._get_current_object())\n    __lt__ = lambda x, o: x._get_current_object() < o\n    __le__ = lambda x, o: x._get_current_object() <= o\n    __eq__ = lambda x, o: x._get_current_object() == o\n    __ne__ = lambda x, o: x._get_current_object() != o\n    __gt__ = lambda x, o: x._get_current_object() > o\n    __ge__ = lambda x, o: x._get_current_object() >= o\n    __cmp__ = lambda x, o: cmp(x._get_current_object(), o)\n    __hash__ = lambda x: hash(x._get_current_object())\n    __call__ = lambda x, *a, **kw: x._get_current_object()(*a, **kw)\n    __len__ = lambda x: len(x._get_current_object())\n    __getitem__ = lambda x, i: x._get_current_object()[i]\n    __iter__ = lambda x: iter(x._get_current_object())\n    __contains__ = lambda x, i: i in x._get_current_object()\n    __getslice__ = lambda x, i, j: x._get_current_object()[i:j]\n    __add__ = lambda x, o: x._get_current_object() + o\n    __sub__ = lambda x, o: x._get_current_object() - o\n    __mul__ = lambda x, o: x._get_current_object() * o\n    __floordiv__ = lambda x, o: x._get_current_object() // o\n    __mod__ = lambda x, o: x._get_current_object() % o\n    __divmod__ = lambda x, o: x._get_current_object().__divmod__(o)\n    __pow__ = lambda x, o: x._get_current_object() ** o\n    __lshift__ = lambda x, o: x._get_current_object() << o\n    __rshift__ = lambda x, o: x._get_current_object() >> o\n    __and__ = lambda x, o: x._get_current_object() & o\n    __xor__ = lambda x, o: x._get_current_object() ^ o\n    __or__ = lambda x, o: x._get_current_object() | o\n    __div__ = lambda x, o: x._get_current_object().__div__(o)\n    __truediv__ = lambda x, o: x._get_current_object().__truediv__(o)\n    __neg__ = lambda x: -(x._get_current_object())\n    __pos__ = lambda x: +(x._get_current_object())\n    __abs__ = lambda x: abs(x._get_current_object())\n    __invert__ = lambda x: ~(x._get_current_object())\n    __complex__ = lambda x: complex(x._get_current_object())\n    __int__ = lambda x: int(x._get_current_object())\n    __long__ = lambda x: long(x._get_current_object())\n    __float__ = lambda x: float(x._get_current_object())\n    __oct__ = lambda x: oct(x._get_current_object())\n    __hex__ = lambda x: hex(x._get_current_object())\n    __index__ = lambda x: x._get_current_object().__index__()\n    __coerce__ = lambda x, o: x._get_current_object().__coerce__(x, o)\n    __enter__ = lambda x: x._get_current_object().__enter__()\n    __exit__ = lambda x, *a, **kw: x._get_current_object().__exit__(*a, **kw)\n"
                }, 
                {
                    "type": "text", 
                    "name": "logutils.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\n\n# Make sure that dictConfig is available\n# This was added in Python 2.7/3.2\ntry:\n    from logging.config import dictConfig\nexcept ImportError:\n    from rq.compat.dictconfig import dictConfig  # noqa\n\n\ndef setup_loghandlers(level=None):\n    if not logging._handlers:\n        dictConfig({\n            'version': 1,\n            'disable_existing_loggers': False,\n\n            'formatters': {\n                'console': {\n                    'format': '%(asctime)s %(message)s',\n                    'datefmt': '%H:%M:%S',\n                },\n            },\n\n            'handlers': {\n                'console': {\n                    'level': 'DEBUG',\n                    # 'class': 'logging.StreamHandler',\n                    'class': 'rq.utils.ColorizingStreamHandler',\n                    'formatter': 'console',\n                    'exclude': ['%(asctime)s'],\n                },\n            },\n\n            'root': {\n                'handlers': ['console'],\n                'level': level or 'INFO',\n            }\n        })\n"
                }, 
                {
                    "type": "text", 
                    "name": "queue.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport uuid\n\nfrom .connections import resolve_connection\nfrom .job import Job, Status\nfrom .utils import utcnow\n\nfrom .exceptions import (DequeueTimeout, InvalidJobOperationError,\n                         NoSuchJobError, UnpickleError)\nfrom .compat import total_ordering, string_types, as_text\n\nfrom redis import WatchError\n\n\ndef get_failed_queue(connection=None):\n    \"\"\"Returns a handle to the special failed queue.\"\"\"\n    return FailedQueue(connection=connection)\n\n\ndef compact(lst):\n    return [item for item in lst if item is not None]\n\n\n@total_ordering\nclass Queue(object):\n    job_class = Job\n    DEFAULT_TIMEOUT = 180  # Default timeout seconds.\n    redis_queue_namespace_prefix = 'rq:queue:'\n    redis_queues_keys = 'rq:queues'\n\n    @classmethod\n    def all(cls, connection=None):\n        \"\"\"Returns an iterable of all Queues.\n        \"\"\"\n        connection = resolve_connection(connection)\n\n        def to_queue(queue_key):\n            return cls.from_queue_key(as_text(queue_key),\n                                      connection=connection)\n        return [to_queue(rq_key) for rq_key in connection.smembers(cls.redis_queues_keys) if rq_key]\n\n    @classmethod\n    def from_queue_key(cls, queue_key, connection=None):\n        \"\"\"Returns a Queue instance, based on the naming conventions for naming\n        the internal Redis keys.  Can be used to reverse-lookup Queues by their\n        Redis keys.\n        \"\"\"\n        prefix = cls.redis_queue_namespace_prefix\n        if not queue_key.startswith(prefix):\n            raise ValueError('Not a valid RQ queue key: %s' % (queue_key,))\n        name = queue_key[len(prefix):]\n        return cls(name, connection=connection)\n\n    def __init__(self, name='default', default_timeout=None, connection=None,\n                 async=True):\n        self.connection = resolve_connection(connection)\n        prefix = self.redis_queue_namespace_prefix\n        self.name = name\n        self._key = '%s%s' % (prefix, name)\n        self._default_timeout = default_timeout\n        self._async = async\n\n    @property\n    def key(self):\n        \"\"\"Returns the Redis key for this Queue.\"\"\"\n        return self._key\n\n    def empty(self):\n        \"\"\"Removes all messages on the queue.\"\"\"\n        script = b\"\"\"\n            local prefix = \"rq:job:\"\n            local q = KEYS[1]\n            local count = 0\n            while true do\n                local job_id = redis.call(\"lpop\", q)\n                if job_id == false then\n                    break\n                end\n\n                -- Delete the relevant keys\n                redis.call(\"del\", prefix..job_id)\n                redis.call(\"del\", prefix..job_id..\":dependents\")\n                count = count + 1\n            end\n            return count\n        \"\"\"\n        script = self.connection.register_script(script)\n        return script(keys=[self.key])\n\n    def is_empty(self):\n        \"\"\"Returns whether the current queue is empty.\"\"\"\n        return self.count == 0\n\n    def fetch_job(self, job_id):\n        try:\n            return self.job_class.fetch(job_id, connection=self.connection)\n        except NoSuchJobError:\n            self.remove(job_id)\n\n    def get_job_ids(self, offset=0, length=-1):\n        \"\"\"Returns a slice of job IDs in the queue.\"\"\"\n        start = offset\n        if length >= 0:\n            end = offset + (length - 1)\n        else:\n            end = length\n        return [as_text(job_id) for job_id in\n                self.connection.lrange(self.key, start, end)]\n\n    def get_jobs(self, offset=0, length=-1):\n        \"\"\"Returns a slice of jobs in the queue.\"\"\"\n        job_ids = self.get_job_ids(offset, length)\n        return compact([self.fetch_job(job_id) for job_id in job_ids])\n\n    @property\n    def job_ids(self):\n        \"\"\"Returns a list of all job IDS in the queue.\"\"\"\n        return self.get_job_ids()\n\n    @property\n    def jobs(self):\n        \"\"\"Returns a list of all (valid) jobs in the queue.\"\"\"\n        return self.get_jobs()\n\n    @property\n    def count(self):\n        \"\"\"Returns a count of all messages in the queue.\"\"\"\n        return self.connection.llen(self.key)\n\n    def remove(self, job_or_id):\n        \"\"\"Removes Job from queue, accepts either a Job instance or ID.\"\"\"\n        job_id = job_or_id.id if isinstance(job_or_id, self.job_class) else job_or_id\n        return self.connection._lrem(self.key, 0, job_id)\n\n    def compact(self):\n        \"\"\"Removes all \"dead\" jobs from the queue by cycling through it, while\n        guarantueeing FIFO semantics.\n        \"\"\"\n        COMPACT_QUEUE = 'rq:queue:_compact:{0}'.format(uuid.uuid4())\n\n        self.connection.rename(self.key, COMPACT_QUEUE)\n        while True:\n            job_id = as_text(self.connection.lpop(COMPACT_QUEUE))\n            if job_id is None:\n                break\n            if self.job_class.exists(job_id, self.connection):\n                self.connection.rpush(self.key, job_id)\n\n    def push_job_id(self, job_id):\n        \"\"\"Pushes a job ID on the corresponding Redis queue.\"\"\"\n        self.connection.rpush(self.key, job_id)\n\n    def enqueue_call(self, func, args=None, kwargs=None, timeout=None,\n                     result_ttl=None, description=None, depends_on=None):\n        \"\"\"Creates a job to represent the delayed function call and enqueues\n        it.\n\n        It is much like `.enqueue()`, except that it takes the function's args\n        and kwargs as explicit arguments.  Any kwargs passed to this function\n        contain options for RQ itself.\n        \"\"\"\n        timeout = timeout or self._default_timeout\n\n        # TODO: job with dependency shouldn't have \"queued\" as status\n        job = self.job_class.create(func, args, kwargs, connection=self.connection,\n                                    result_ttl=result_ttl, status=Status.QUEUED,\n                                    description=description, depends_on=depends_on, timeout=timeout)\n\n        # If job depends on an unfinished job, register itself on it's\n        # parent's dependents instead of enqueueing it.\n        # If WatchError is raised in the process, that means something else is\n        # modifying the dependency. In this case we simply retry\n        if depends_on is not None:\n            with self.connection.pipeline() as pipe:\n                while True:\n                    try:\n                        pipe.watch(depends_on.key)\n                        if depends_on.get_status() != Status.FINISHED:\n                            job.register_dependency()\n                            job.save()\n                            return job\n                        break\n                    except WatchError:\n                        continue\n\n        return self.enqueue_job(job)\n\n    def enqueue(self, f, *args, **kwargs):\n        \"\"\"Creates a job to represent the delayed function call and enqueues\n        it.\n\n        Expects the function to call, along with the arguments and keyword\n        arguments.\n\n        The function argument `f` may be any of the following:\n\n        * A reference to a function\n        * A reference to an object's instance method\n        * A string, representing the location of a function (must be\n          meaningful to the import context of the workers)\n        \"\"\"\n        if not isinstance(f, string_types) and f.__module__ == '__main__':\n            raise ValueError('Functions from the __main__ module cannot be processed '\n                             'by workers.')\n\n        # Detect explicit invocations, i.e. of the form:\n        #     q.enqueue(foo, args=(1, 2), kwargs={'a': 1}, timeout=30)\n        timeout = kwargs.pop('timeout', None)\n        description = kwargs.pop('description', None)\n        result_ttl = kwargs.pop('result_ttl', None)\n        depends_on = kwargs.pop('depends_on', None)\n\n        if 'args' in kwargs or 'kwargs' in kwargs:\n            assert args == (), 'Extra positional arguments cannot be used when using explicit args and kwargs.'  # noqa\n            args = kwargs.pop('args', None)\n            kwargs = kwargs.pop('kwargs', None)\n\n        return self.enqueue_call(func=f, args=args, kwargs=kwargs,\n                                 timeout=timeout, result_ttl=result_ttl,\n                                 description=description, depends_on=depends_on)\n\n    def enqueue_job(self, job, set_meta_data=True):\n        \"\"\"Enqueues a job for delayed execution.\n\n        If the `set_meta_data` argument is `True` (default), it will update\n        the properties `origin` and `enqueued_at`.\n\n        If Queue is instantiated with async=False, job is executed immediately.\n        \"\"\"\n        # Add Queue key set\n        self.connection.sadd(self.redis_queues_keys, self.key)\n\n        if set_meta_data:\n            job.origin = self.name\n            job.enqueued_at = utcnow()\n\n        if job.timeout is None:\n            job.timeout = self.DEFAULT_TIMEOUT\n        job.save()\n\n        if self._async:\n            self.push_job_id(job.id)\n        else:\n            job.perform()\n            job.save()\n        return job\n\n    def enqueue_dependents(self, job):\n        \"\"\"Enqueues all jobs in the given job's dependents set and clears it.\"\"\"\n        # TODO: can probably be pipelined\n        while True:\n            job_id = as_text(self.connection.spop(job.dependents_key))\n            if job_id is None:\n                break\n            dependent = self.job_class.fetch(job_id, connection=self.connection)\n            self.enqueue_job(dependent)\n\n    def pop_job_id(self):\n        \"\"\"Pops a given job ID from this Redis queue.\"\"\"\n        return as_text(self.connection.lpop(self.key))\n\n    @classmethod\n    def lpop(cls, queue_keys, timeout, connection=None):\n        \"\"\"Helper method.  Intermediate method to abstract away from some\n        Redis API details, where LPOP accepts only a single key, whereas BLPOP\n        accepts multiple.  So if we want the non-blocking LPOP, we need to\n        iterate over all queues, do individual LPOPs, and return the result.\n\n        Until Redis receives a specific method for this, we'll have to wrap it\n        this way.\n\n        The timeout parameter is interpreted as follows:\n            None - non-blocking (return immediately)\n             > 0 - maximum number of seconds to block\n        \"\"\"\n        connection = resolve_connection(connection)\n        if timeout is not None:  # blocking variant\n            if timeout == 0:\n                raise ValueError('RQ does not support indefinite timeouts. Please pick a timeout value > 0.')\n            result = connection.blpop(queue_keys, timeout)\n            if result is None:\n                raise DequeueTimeout(timeout, queue_keys)\n            queue_key, job_id = result\n            return queue_key, job_id\n        else:  # non-blocking variant\n            for queue_key in queue_keys:\n                blob = connection.lpop(queue_key)\n                if blob is not None:\n                    return queue_key, blob\n            return None\n\n    def dequeue(self):\n        \"\"\"Dequeues the front-most job from this queue.\n\n        Returns a job_class instance, which can be executed or inspected.\n        \"\"\"\n        job_id = self.pop_job_id()\n        if job_id is None:\n            return None\n        try:\n            job = self.job_class.fetch(job_id, connection=self.connection)\n        except NoSuchJobError as e:\n            # Silently pass on jobs that don't exist (anymore),\n            # and continue by reinvoking itself recursively\n            return self.dequeue()\n        except UnpickleError as e:\n            # Attach queue information on the exception for improved error\n            # reporting\n            e.job_id = job_id\n            e.queue = self\n            raise e\n        return job\n\n    @classmethod\n    def dequeue_any(cls, queues, timeout, connection=None):\n        \"\"\"Class method returning the job_class instance at the front of the given\n        set of Queues, where the order of the queues is important.\n\n        When all of the Queues are empty, depending on the `timeout` argument,\n        either blocks execution of this function for the duration of the\n        timeout or until new messages arrive on any of the queues, or returns\n        None.\n\n        See the documentation of cls.lpop for the interpretation of timeout.\n        \"\"\"\n        queue_keys = [q.key for q in queues]\n        result = cls.lpop(queue_keys, timeout, connection=connection)\n        if result is None:\n            return None\n        queue_key, job_id = map(as_text, result)\n        queue = cls.from_queue_key(queue_key, connection=connection)\n        try:\n            job = cls.job_class.fetch(job_id, connection=connection)\n        except NoSuchJobError:\n            # Silently pass on jobs that don't exist (anymore),\n            # and continue by reinvoking the same function recursively\n            return cls.dequeue_any(queues, timeout, connection=connection)\n        except UnpickleError as e:\n            # Attach queue information on the exception for improved error\n            # reporting\n            e.job_id = job_id\n            e.queue = queue\n            raise e\n        return job, queue\n\n    # Total ordering defition (the rest of the required Python methods are\n    # auto-generated by the @total_ordering decorator)\n    def __eq__(self, other):  # noqa\n        if not isinstance(other, Queue):\n            raise TypeError('Cannot compare queues to other objects.')\n        return self.name == other.name\n\n    def __lt__(self, other):\n        if not isinstance(other, Queue):\n            raise TypeError('Cannot compare queues to other objects.')\n        return self.name < other.name\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __repr__(self):  # noqa\n        return 'Queue(%r)' % (self.name,)\n\n    def __str__(self):\n        return '<Queue \\'%s\\'>' % (self.name,)\n\n\nclass FailedQueue(Queue):\n    def __init__(self, connection=None):\n        super(FailedQueue, self).__init__(Status.FAILED, connection=connection)\n\n    def quarantine(self, job, exc_info):\n        \"\"\"Puts the given Job in quarantine (i.e. put it on the failed\n        queue).\n\n        This is different from normal job enqueueing, since certain meta data\n        must not be overridden (e.g. `origin` or `enqueued_at`) and other meta\n        data must be inserted (`ended_at` and `exc_info`).\n        \"\"\"\n        job.ended_at = utcnow()\n        job.exc_info = exc_info\n        return self.enqueue_job(job, set_meta_data=False)\n\n    def requeue(self, job_id):\n        \"\"\"Requeues the job with the given job ID.\"\"\"\n        try:\n            job = self.job_class.fetch(job_id, connection=self.connection)\n        except NoSuchJobError:\n            # Silently ignore/remove this job and return (i.e. do nothing)\n            self.remove(job_id)\n            return\n\n        # Delete it from the failed queue (raise an error if that failed)\n        if self.remove(job) == 0:\n            raise InvalidJobOperationError('Cannot requeue non-failed jobs.')\n\n        job.set_status(Status.QUEUED)\n        job.exc_info = None\n        q = Queue(job.origin, connection=self.connection)\n        q.enqueue_job(job)\n"
                }, 
                {
                    "type": "dir", 
                    "name": "scripts", 
                    "contents": [
                        {
                            "type": "text", 
                            "name": "__init__.py", 
                            "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport importlib\nimport os\nfrom functools import partial\nfrom warnings import warn\n\nimport redis\nfrom rq import use_connection\nfrom rq.utils import first\n\n\ndef add_standard_arguments(parser):\n    parser.add_argument('--config', '-c', default=None,\n                        help='Module containing RQ settings.')\n    parser.add_argument('--url', '-u', default=None,\n                        help='URL describing Redis connection details. '\n                             'Overrides other connection arguments if supplied.')\n    parser.add_argument('--host', '-H', default=None,\n                        help='The Redis hostname (default: localhost)')\n    parser.add_argument('--port', '-p', default=None,\n                        help='The Redis portnumber (default: 6379)')\n    parser.add_argument('--db', '-d', type=int, default=None,\n                        help='The Redis database (default: 0)')\n    parser.add_argument('--password', '-a', default=None,\n                        help='The Redis password (default: None)')\n    parser.add_argument('--socket', '-s', default=None,\n                        help='The Redis Unix socket')\n\n\ndef read_config_file(module):\n    \"\"\"Reads all UPPERCASE variables defined in the given module file.\"\"\"\n    settings = importlib.import_module(module)\n    return dict([(k, v)\n                 for k, v in settings.__dict__.items()\n                 if k.upper() == k])\n\n\ndef setup_default_arguments(args, settings):\n    \"\"\" Sets up args from settings or defaults \"\"\"\n    args.url = first([args.url, settings.get('REDIS_URL'), os.environ.get('RQ_REDIS_URL')])\n\n    if (args.host or args.port or args.socket or args.db or args.password):\n        warn('Host, port, db, password options for Redis will not be '\n             'supported in future versions of RQ. '\n             'Please use `REDIS_URL` or `--url` instead.', DeprecationWarning)\n\n    strict_first = partial(first, key=lambda obj: obj is not None)\n\n    args.host = strict_first([args.host, settings.get('REDIS_HOST'), os.environ.get('RQ_REDIS_HOST'), 'localhost'])\n    args.port = int(strict_first([args.port, settings.get('REDIS_PORT'), os.environ.get('RQ_REDIS_PORT'), 6379]))\n    args.socket = strict_first([args.socket, settings.get('REDIS_SOCKET'), os.environ.get('RQ_REDIS_SOCKET'), None])\n    args.db = strict_first([args.db, settings.get('REDIS_DB'), os.environ.get('RQ_REDIS_DB'), 0])\n    args.password = strict_first([args.password, settings.get('REDIS_PASSWORD'), os.environ.get('RQ_REDIS_PASSWORD')])\n\n\ndef setup_redis(args):\n    if args.url is not None:\n        redis_conn = redis.StrictRedis.from_url(args.url)\n    else:\n        redis_conn = redis.StrictRedis(host=args.host, port=args.port, db=args.db,\n                                       password=args.password, unix_socket_path=args.socket)\n    use_connection(redis_conn)\n"
                        }, 
                        {
                            "type": "exec", 
                            "name": "rqgenload.py", 
                            "contents": "#!/usr/bin/env python", 
                            "description": "# -*- coding: utf-8 -*-"
                        }, 
                        {
                            "type": "exec", 
                            "name": "rqinfo.py", 
                            "contents": "#!/usr/bin/env python", 
                            "description": "# -*- coding: utf-8 -*-"
                        }, 
                        {
                            "type": "text", 
                            "name": "rqworker.py", 
                            "contents": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport argparse\nimport logging\nimport logging.config\nimport os\nimport sys\n\nfrom redis.exceptions import ConnectionError\nfrom rq import Queue\nfrom rq.contrib.legacy import cleanup_ghosts\nfrom rq.logutils import setup_loghandlers\nfrom rq.scripts import (add_standard_arguments, read_config_file,\n                        setup_default_arguments, setup_redis)\nfrom rq.utils import import_attribute\n\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Starts an RQ worker.')\n    add_standard_arguments(parser)\n\n    parser.add_argument('--burst', '-b', action='store_true', default=False, help='Run in burst mode (quit after all work is done)')  # noqa\n    parser.add_argument('--name', '-n', default=None, help='Specify a different name')\n    parser.add_argument('--worker-class', '-w', action='store', default='rq.Worker', help='RQ Worker class to use')\n    parser.add_argument('--path', '-P', default='.', help='Specify the import path.')\n    parser.add_argument('--results-ttl', default=None, help='Default results timeout to be used')\n    parser.add_argument('--worker-ttl', default=None, help='Default worker timeout to be used')\n    parser.add_argument('--verbose', '-v', action='store_true', default=False, help='Show more output')\n    parser.add_argument('--quiet', '-q', action='store_true', default=False, help='Show less output')\n    parser.add_argument('--sentry-dsn', action='store', default=None, metavar='URL', help='Report exceptions to this Sentry DSN')  # noqa\n    parser.add_argument('--pid', action='store', default=None,\n                        help='Write the process ID number to a file at the specified path')\n    parser.add_argument('queues', nargs='*', help='The queues to listen on (default: \\'default\\')')\n\n    return parser.parse_args()\n\n\ndef setup_loghandlers_from_args(args):\n    if args.verbose and args.quiet:\n        raise RuntimeError(\"Flags --verbose and --quiet are mutually exclusive.\")\n\n    if args.verbose:\n        level = 'DEBUG'\n    elif args.quiet:\n        level = 'WARNING'\n    else:\n        level = 'INFO'\n    setup_loghandlers(level)\n\n\ndef main():\n    args = parse_args()\n\n    if args.path:\n        sys.path = args.path.split(':') + sys.path\n\n    settings = {}\n    if args.config:\n        settings = read_config_file(args.config)\n\n    setup_default_arguments(args, settings)\n\n    # Worker specific default arguments\n    if not args.queues:\n        args.queues = settings.get('QUEUES', ['default'])\n\n    if args.sentry_dsn is None:\n        args.sentry_dsn = settings.get('SENTRY_DSN',\n                                       os.environ.get('SENTRY_DSN', None))\n\n    if args.pid:\n        with open(os.path.expanduser(args.pid), \"w\") as fp:\n            fp.write(str(os.getpid()))\n\n    setup_loghandlers_from_args(args)\n    setup_redis(args)\n\n    cleanup_ghosts()\n    worker_class = import_attribute(args.worker_class)\n\n    try:\n        queues = list(map(Queue, args.queues))\n        w = worker_class(queues,\n                         name=args.name,\n                         default_worker_ttl=args.worker_ttl,\n                         default_result_ttl=args.results_ttl)\n\n        # Should we configure Sentry?\n        if args.sentry_dsn:\n            from raven import Client\n            from rq.contrib.sentry import register_sentry\n            client = Client(args.sentry_dsn)\n            register_sentry(client, w)\n\n        w.work(burst=args.burst)\n    except ConnectionError as e:\n        print(e)\n        sys.exit(1)\n"
                        }
                    ]
                }, 
                {
                    "type": "text", 
                    "name": "timeouts.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport signal\n\n\nclass JobTimeoutException(Exception):\n    \"\"\"Raised when a job takes longer to complete than the allowed maximum\n    timeout value.\n    \"\"\"\n    pass\n\n\nclass BaseDeathPenalty(object):\n    \"\"\"Base class to setup job timeouts.\"\"\"\n\n    def __init__(self, timeout):\n        self._timeout = timeout\n\n    def __enter__(self):\n        self.setup_death_penalty()\n\n    def __exit__(self, type, value, traceback):\n        # Always cancel immediately, since we're done\n        try:\n            self.cancel_death_penalty()\n        except JobTimeoutException:\n            # Weird case: we're done with the with body, but now the alarm is\n            # fired.  We may safely ignore this situation and consider the\n            # body done.\n            pass\n\n        # __exit__ may return True to supress further exception handling.  We\n        # don't want to suppress any exceptions here, since all errors should\n        # just pass through, JobTimeoutException being handled normally to the\n        # invoking context.\n        return False\n\n    def setup_death_penalty(self):\n        raise NotImplementedError()\n\n    def cancel_death_penalty(self):\n        raise NotImplementedError()\n\n\nclass UnixSignalDeathPenalty(BaseDeathPenalty):\n\n    def handle_death_penalty(self, signum, frame):\n        raise JobTimeoutException('Job exceeded maximum timeout '\n                                  'value (%d seconds).' % self._timeout)\n\n    def setup_death_penalty(self):\n        \"\"\"Sets up an alarm signal and a signal handler that raises\n        a JobTimeoutException after the timeout amount (expressed in\n        seconds).\n        \"\"\"\n        signal.signal(signal.SIGALRM, self.handle_death_penalty)\n        signal.alarm(self._timeout)\n\n    def cancel_death_penalty(self):\n        \"\"\"Removes the death penalty alarm and puts back the system into\n        default signal handling.\n        \"\"\"\n        signal.alarm(0)\n        signal.signal(signal.SIGALRM, signal.SIG_DFL)\n"
                }, 
                {
                    "type": "text", 
                    "name": "utils.py", 
                    "contents": "# -*- coding: utf-8 -*-\n\"\"\"\nMiscellaneous helper functions.\n\nThe formatter for ANSI colored console output is heavily based on Pygments\nterminal colorizing code, originally by Georg Brandl.\n\"\"\"\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport importlib\nimport datetime\nimport logging\nimport os\nimport sys\n\nfrom .compat import is_python_version\n\n\ndef gettermsize():\n    def ioctl_GWINSZ(fd):\n        try:\n            import fcntl\n            import struct\n            import termios\n            cr = struct.unpack('hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))\n        except:\n            return None\n        return cr\n    cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)\n    if not cr:\n        try:\n            fd = os.open(os.ctermid(), os.O_RDONLY)\n            cr = ioctl_GWINSZ(fd)\n            os.close(fd)\n        except:\n            pass\n    if not cr:\n        try:\n            cr = (os.environ['LINES'], os.environ['COLUMNS'])\n        except:\n            cr = (25, 80)\n    return int(cr[1]), int(cr[0])\n\n\nclass _Colorizer(object):\n    def __init__(self):\n        esc = \"\\x1b[\"\n\n        self.codes = {}\n        self.codes[\"\"] = \"\"\n        self.codes[\"reset\"] = esc + \"39;49;00m\"\n\n        self.codes[\"bold\"] = esc + \"01m\"\n        self.codes[\"faint\"] = esc + \"02m\"\n        self.codes[\"standout\"] = esc + \"03m\"\n        self.codes[\"underline\"] = esc + \"04m\"\n        self.codes[\"blink\"] = esc + \"05m\"\n        self.codes[\"overline\"] = esc + \"06m\"\n\n        dark_colors = [\"black\", \"darkred\", \"darkgreen\", \"brown\", \"darkblue\",\n                       \"purple\", \"teal\", \"lightgray\"]\n        light_colors = [\"darkgray\", \"red\", \"green\", \"yellow\", \"blue\",\n                        \"fuchsia\", \"turquoise\", \"white\"]\n\n        x = 30\n        for d, l in zip(dark_colors, light_colors):\n            self.codes[d] = esc + \"%im\" % x\n            self.codes[l] = esc + \"%i;01m\" % x\n            x += 1\n\n        del d, l, x\n\n        self.codes[\"darkteal\"] = self.codes[\"turquoise\"]\n        self.codes[\"darkyellow\"] = self.codes[\"brown\"]\n        self.codes[\"fuscia\"] = self.codes[\"fuchsia\"]\n        self.codes[\"white\"] = self.codes[\"bold\"]\n\n        if hasattr(sys.stdout, \"isatty\"):\n            self.notty = not sys.stdout.isatty()\n        else:\n            self.notty = True\n\n    def reset_color(self):\n        return self.codes[\"reset\"]\n\n    def colorize(self, color_key, text):\n        if not sys.stdout.isatty():\n            return text\n        else:\n            return self.codes[color_key] + text + self.codes[\"reset\"]\n\n    def ansiformat(self, attr, text):\n        \"\"\"\n        Format ``text`` with a color and/or some attributes::\n\n            color       normal color\n            *color*     bold color\n            _color_     underlined color\n            +color+     blinking color\n        \"\"\"\n        result = []\n        if attr[:1] == attr[-1:] == '+':\n            result.append(self.codes['blink'])\n            attr = attr[1:-1]\n        if attr[:1] == attr[-1:] == '*':\n            result.append(self.codes['bold'])\n            attr = attr[1:-1]\n        if attr[:1] == attr[-1:] == '_':\n            result.append(self.codes['underline'])\n            attr = attr[1:-1]\n        result.append(self.codes[attr])\n        result.append(text)\n        result.append(self.codes['reset'])\n        return ''.join(result)\n\n\ncolorizer = _Colorizer()\n\n\ndef make_colorizer(color):\n    \"\"\"Creates a function that colorizes text with the given color.\n\n    For example:\n\n        green = make_colorizer('darkgreen')\n        red = make_colorizer('red')\n\n    Then, you can use:\n\n        print \"It's either \" + green('OK') + ' or ' + red('Oops')\n    \"\"\"\n    def inner(text):\n        return colorizer.colorize(color, text)\n    return inner\n\n\nclass ColorizingStreamHandler(logging.StreamHandler):\n\n    levels = {\n        logging.WARNING: make_colorizer('darkyellow'),\n        logging.ERROR: make_colorizer('darkred'),\n        logging.CRITICAL: make_colorizer('darkred'),\n    }\n\n    def __init__(self, exclude=None, *args, **kwargs):\n        self.exclude = exclude\n        if is_python_version((2, 6)):\n            logging.StreamHandler.__init__(self, *args, **kwargs)\n        else:\n            super(ColorizingStreamHandler, self).__init__(*args, **kwargs)\n\n    @property\n    def is_tty(self):\n        isatty = getattr(self.stream, 'isatty', None)\n        return isatty and isatty()\n\n    def format(self, record):\n        message = logging.StreamHandler.format(self, record)\n        if self.is_tty:\n            colorize = self.levels.get(record.levelno, lambda x: x)\n\n            # Don't colorize any traceback\n            parts = message.split('\\n', 1)\n            parts[0] = \" \".join([parts[0].split(\" \", 1)[0], colorize(parts[0].split(\" \", 1)[1])])\n\n            message = '\\n'.join(parts)\n\n        return message\n\n\ndef import_attribute(name):\n    \"\"\"Return an attribute from a dotted path name (e.g. \"path.to.func\").\"\"\"\n    module_name, attribute = name.rsplit('.', 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, attribute)\n\n\ndef utcnow():\n    return datetime.datetime.utcnow()\n\n\ndef utcformat(dt):\n    return dt.strftime(u'%Y-%m-%dT%H:%M:%SZ')\n\n\ndef utcparse(string):\n    try:\n        return datetime.datetime.strptime(string, '%Y-%m-%dT%H:%M:%SZ')\n    except ValueError:\n        # This catches RQ < 0.4 datetime format\n        return datetime.datetime.strptime(string, '%Y-%m-%dT%H:%M:%S.%f+00:00')\n\n\ndef first(iterable, default=None, key=None):\n    \"\"\"\n    Return first element of `iterable` that evaluates true, else return None\n    (or an optional default value).\n\n    >>> first([0, False, None, [], (), 42])\n    42\n\n    >>> first([0, False, None, [], ()]) is None\n    True\n\n    >>> first([0, False, None, [], ()], default='ohai')\n    'ohai'\n\n    >>> import re\n    >>> m = first(re.match(regex, 'abc') for regex in ['b.*', 'a(.*)'])\n    >>> m.group(1)\n    'bc'\n\n    The optional `key` argument specifies a one-argument predicate function\n    like that used for `filter()`.  The `key` argument, if supplied, must be\n    in keyword form.  For example:\n\n    >>> first([1, 1, 3, 4, 5], key=lambda x: x % 2 == 0)\n    4\n\n    \"\"\"\n    if key is None:\n        for el in iterable:\n            if el:\n                return el\n    else:\n        for el in iterable:\n            if key(el):\n                return el\n\n    return default\n"
                }, 
                {
                    "type": "text", 
                    "name": "version.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nVERSION = '0.4.6'\n"
                }, 
                {
                    "type": "text", 
                    "name": "worker.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport errno\nimport logging\nimport os\nimport random\nimport signal\nimport socket\nimport sys\nimport time\nimport traceback\n\nfrom rq.compat import as_text, text_type\n\nfrom .connections import get_current_connection\nfrom .exceptions import DequeueTimeout, NoQueueError\nfrom .job import Job, Status\nfrom .logutils import setup_loghandlers\nfrom .queue import get_failed_queue, Queue\nfrom .timeouts import UnixSignalDeathPenalty\nfrom .utils import make_colorizer, utcformat, utcnow\nfrom .version import VERSION\n\ntry:\n    from procname import setprocname\nexcept ImportError:\n    def setprocname(*args, **kwargs):  # noqa\n        pass\n\ngreen = make_colorizer('darkgreen')\nyellow = make_colorizer('darkyellow')\nblue = make_colorizer('darkblue')\n\nDEFAULT_WORKER_TTL = 420\nDEFAULT_RESULT_TTL = 500\nlogger = logging.getLogger(__name__)\n\n\nclass StopRequested(Exception):\n    pass\n\n\ndef iterable(x):\n    return hasattr(x, '__iter__')\n\n\ndef compact(l):\n    return [x for x in l if x is not None]\n\n_signames = dict((getattr(signal, signame), signame)\n                 for signame in dir(signal)\n                 if signame.startswith('SIG') and '_' not in signame)\n\n\ndef signal_name(signum):\n    # Hackety-hack-hack: is there really no better way to reverse lookup the\n    # signal name?  If you read this and know a way: please provide a patch :)\n    try:\n        return _signames[signum]\n    except KeyError:\n        return 'SIG_UNKNOWN'\n\n\nclass Worker(object):\n    redis_worker_namespace_prefix = 'rq:worker:'\n    redis_workers_keys = 'rq:workers'\n    death_penalty_class = UnixSignalDeathPenalty\n    queue_class = Queue\n    job_class = Job\n\n    @classmethod\n    def all(cls, connection=None):\n        \"\"\"Returns an iterable of all Workers.\n        \"\"\"\n        if connection is None:\n            connection = get_current_connection()\n        reported_working = connection.smembers(cls.redis_workers_keys)\n        workers = [cls.find_by_key(as_text(key), connection)\n                   for key in reported_working]\n        return compact(workers)\n\n    @classmethod\n    def find_by_key(cls, worker_key, connection=None):\n        \"\"\"Returns a Worker instance, based on the naming conventions for\n        naming the internal Redis keys.  Can be used to reverse-lookup Workers\n        by their Redis keys.\n        \"\"\"\n        prefix = cls.redis_worker_namespace_prefix\n        if not worker_key.startswith(prefix):\n            raise ValueError('Not a valid RQ worker key: %s' % (worker_key,))\n\n        if connection is None:\n            connection = get_current_connection()\n        if not connection.exists(worker_key):\n            connection.srem(cls.redis_workers_keys, worker_key)\n            return None\n\n        name = worker_key[len(prefix):]\n        worker = cls([], name, connection=connection)\n        queues = as_text(connection.hget(worker.key, 'queues'))\n        worker._state = connection.hget(worker.key, 'state') or '?'\n        worker._job_id = connection.hget(worker.key, 'current_job') or None\n        if queues:\n            worker.queues = [cls.queue_class(queue, connection=connection)\n                             for queue in queues.split(',')]\n        return worker\n\n    def __init__(self, queues, name=None,\n                 default_result_ttl=None, connection=None,\n                 exc_handler=None, default_worker_ttl=None):  # noqa\n        if connection is None:\n            connection = get_current_connection()\n        self.connection = connection\n        if isinstance(queues, self.queue_class):\n            queues = [queues]\n        self._name = name\n        self.queues = queues\n        self.validate_queues()\n        self._exc_handlers = []\n\n        if default_result_ttl is None:\n            default_result_ttl = DEFAULT_RESULT_TTL\n        self.default_result_ttl = default_result_ttl\n\n        if default_worker_ttl is None:\n            default_worker_ttl = DEFAULT_WORKER_TTL\n        self.default_worker_ttl = default_worker_ttl\n\n        self._state = 'starting'\n        self._is_horse = False\n        self._horse_pid = 0\n        self._stopped = False\n        self.log = logger\n        self.failed_queue = get_failed_queue(connection=self.connection)\n\n        # By default, push the \"move-to-failed-queue\" exception handler onto\n        # the stack\n        self.push_exc_handler(self.move_to_failed_queue)\n        if exc_handler is not None:\n            self.push_exc_handler(exc_handler)\n\n    def validate_queues(self):\n        \"\"\"Sanity check for the given queues.\"\"\"\n        if not iterable(self.queues):\n            raise ValueError('Argument queues not iterable.')\n        for queue in self.queues:\n            if not isinstance(queue, self.queue_class):\n                raise NoQueueError('Give each worker at least one Queue.')\n\n    def queue_names(self):\n        \"\"\"Returns the queue names of this worker's queues.\"\"\"\n        return map(lambda q: q.name, self.queues)\n\n    def queue_keys(self):\n        \"\"\"Returns the Redis keys representing this worker's queues.\"\"\"\n        return map(lambda q: q.key, self.queues)\n\n    @property\n    def name(self):\n        \"\"\"Returns the name of the worker, under which it is registered to the\n        monitoring system.\n\n        By default, the name of the worker is constructed from the current\n        (short) host name and the current PID.\n        \"\"\"\n        if self._name is None:\n            hostname = socket.gethostname()\n            shortname, _, _ = hostname.partition('.')\n            self._name = '%s.%s' % (shortname, self.pid)\n        return self._name\n\n    @property\n    def key(self):\n        \"\"\"Returns the worker's Redis hash key.\"\"\"\n        return self.redis_worker_namespace_prefix + self.name\n\n    @property\n    def pid(self):\n        \"\"\"The current process ID.\"\"\"\n        return os.getpid()\n\n    @property\n    def horse_pid(self):\n        \"\"\"The horse's process ID.  Only available in the worker.  Will return\n        0 in the horse part of the fork.\n        \"\"\"\n        return self._horse_pid\n\n    @property\n    def is_horse(self):\n        \"\"\"Returns whether or not this is the worker or the work horse.\"\"\"\n        return self._is_horse\n\n    def procline(self, message):\n        \"\"\"Changes the current procname for the process.\n\n        This can be used to make `ps -ef` output more readable.\n        \"\"\"\n        setprocname('rq: %s' % (message,))\n\n    def register_birth(self):\n        \"\"\"Registers its own birth.\"\"\"\n        self.log.debug('Registering birth of worker %s' % (self.name,))\n        if self.connection.exists(self.key) and \\\n                not self.connection.hexists(self.key, 'death'):\n            raise ValueError('There exists an active worker named \\'%s\\' '\n                             'already.' % (self.name,))\n        key = self.key\n        queues = ','.join(self.queue_names())\n        with self.connection._pipeline() as p:\n            p.delete(key)\n            p.hset(key, 'birth', utcformat(utcnow()))\n            p.hset(key, 'queues', queues)\n            p.sadd(self.redis_workers_keys, key)\n            p.expire(key, self.default_worker_ttl)\n            p.execute()\n\n    def register_death(self):\n        \"\"\"Registers its own death.\"\"\"\n        self.log.debug('Registering death')\n        with self.connection._pipeline() as p:\n            # We cannot use self.state = 'dead' here, because that would\n            # rollback the pipeline\n            p.srem(self.redis_workers_keys, self.key)\n            p.hset(self.key, 'death', utcformat(utcnow()))\n            p.expire(self.key, 60)\n            p.execute()\n\n    def set_state(self, state, pipeline=None):\n        self._state = state\n        connection = pipeline if pipeline is not None else self.connection\n        connection.hset(self.key, 'state', state)\n\n    def _set_state(self, state):\n        \"\"\"Raise a DeprecationWarning if ``worker.state = X`` is used\"\"\"\n        raise DeprecationWarning(\n            \"worker.state is deprecated, use worker.set_state() instead.\"\n        )\n        self.set_state(state)\n\n    def get_state(self):\n        return self._state\n\n    def _get_state(self):\n        \"\"\"Raise a DeprecationWarning if ``worker.state == X`` is used\"\"\"\n        raise DeprecationWarning(\n            \"worker.state is deprecated, use worker.get_state() instead.\"\n        )\n        return self.get_state()\n\n    state = property(_get_state, _set_state)\n\n    def set_current_job_id(self, job_id, pipeline=None):\n        connection = pipeline if pipeline is not None else self.connection\n\n        if job_id is None:\n            connection.hdel(self.key, 'current_job')\n        else:\n            connection.hset(self.key, 'current_job', job_id)\n\n    def get_current_job_id(self, pipeline=None):\n        connection = pipeline if pipeline is not None else self.connection\n        return as_text(connection.hget(self.key, 'current_job'))\n\n    def get_current_job(self):\n        \"\"\"Returns the job id of the currently executing job.\"\"\"\n        job_id = self.get_current_job_id()\n\n        if job_id is None:\n            return None\n\n        return self.job_class.fetch(job_id, self.connection)\n\n    @property\n    def stopped(self):\n        return self._stopped\n\n    def _install_signal_handlers(self):\n        \"\"\"Installs signal handlers for handling SIGINT and SIGTERM\n        gracefully.\n        \"\"\"\n\n        def request_force_stop(signum, frame):\n            \"\"\"Terminates the application (cold shutdown).\n            \"\"\"\n            self.log.warning('Cold shut down.')\n\n            # Take down the horse with the worker\n            if self.horse_pid:\n                msg = 'Taking down horse %d with me.' % self.horse_pid\n                self.log.debug(msg)\n                try:\n                    os.kill(self.horse_pid, signal.SIGKILL)\n                except OSError as e:\n                    # ESRCH (\"No such process\") is fine with us\n                    if e.errno != errno.ESRCH:\n                        self.log.debug('Horse already down.')\n                        raise\n            raise SystemExit()\n\n        def request_stop(signum, frame):\n            \"\"\"Stops the current worker loop but waits for child processes to\n            end gracefully (warm shutdown).\n            \"\"\"\n            self.log.debug('Got signal %s.' % signal_name(signum))\n\n            signal.signal(signal.SIGINT, request_force_stop)\n            signal.signal(signal.SIGTERM, request_force_stop)\n\n            msg = 'Warm shut down requested.'\n            self.log.warning(msg)\n\n            # If shutdown is requested in the middle of a job, wait until\n            # finish before shutting down\n            if self.get_state() == 'busy':\n                self._stopped = True\n                self.log.debug('Stopping after current horse is finished. '\n                               'Press Ctrl+C again for a cold shutdown.')\n            else:\n                raise StopRequested()\n\n        signal.signal(signal.SIGINT, request_stop)\n        signal.signal(signal.SIGTERM, request_stop)\n\n    def work(self, burst=False):\n        \"\"\"Starts the work loop.\n\n        Pops and performs all jobs on the current list of queues.  When all\n        queues are empty, block and wait for new jobs to arrive on any of the\n        queues, unless `burst` mode is enabled.\n\n        The return value indicates whether any jobs were processed.\n        \"\"\"\n        setup_loghandlers()\n        self._install_signal_handlers()\n\n        did_perform_work = False\n        self.register_birth()\n        self.log.info('RQ worker started, version %s' % VERSION)\n        self.set_state('starting')\n        try:\n            while True:\n                if self.stopped:\n                    self.log.info('Stopping on request.')\n                    break\n\n                timeout = None if burst else max(1, self.default_worker_ttl - 60)\n                try:\n                    result = self.dequeue_job_and_maintain_ttl(timeout)\n                    if result is None:\n                        break\n                except StopRequested:\n                    break\n\n                job, queue = result\n                self.execute_job(job)\n                self.heartbeat()\n\n                if job.get_status() == Status.FINISHED:\n                    queue.enqueue_dependents(job)\n\n                did_perform_work = True\n        finally:\n            if not self.is_horse:\n                self.register_death()\n        return did_perform_work\n\n    def dequeue_job_and_maintain_ttl(self, timeout):\n        result = None\n        qnames = self.queue_names()\n\n        self.set_state('idle')\n        self.procline('Listening on %s' % ','.join(qnames))\n        self.log.info('')\n        self.log.info('*** Listening on %s...' %\n                      green(', '.join(qnames)))\n\n        while True:\n            self.heartbeat()\n\n            try:\n                result = self.queue_class.dequeue_any(self.queues, timeout,\n                                                      connection=self.connection)\n                if result is not None:\n                    job, queue = result\n                    self.log.info('%s: %s (%s)' % (green(queue.name),\n                                  blue(job.description), job.id))\n\n                break\n            except DequeueTimeout:\n                pass\n\n        self.heartbeat()\n        return result\n\n    def heartbeat(self, timeout=0):\n        \"\"\"Specifies a new worker timeout, typically by extending the\n        expiration time of the worker, effectively making this a \"heartbeat\"\n        to not expire the worker until the timeout passes.\n\n        The next heartbeat should come before this time, or the worker will\n        die (at least from the monitoring dashboards).\n\n        The effective timeout can never be shorter than default_worker_ttl,\n        only larger.\n        \"\"\"\n        timeout = max(timeout, self.default_worker_ttl)\n        self.connection.expire(self.key, timeout)\n        self.log.debug('Sent heartbeat to prevent worker timeout. '\n                       'Next one should arrive within {0} seconds.'.format(timeout))\n\n    def execute_job(self, job):\n        \"\"\"Spawns a work horse to perform the actual work and passes it a job.\n        The worker will wait for the work horse and make sure it executes\n        within the given timeout bounds, or will end the work horse with\n        SIGALRM.\n        \"\"\"\n        child_pid = os.fork()\n        if child_pid == 0:\n            self.main_work_horse(job)\n        else:\n            self._horse_pid = child_pid\n            self.procline('Forked %d at %d' % (child_pid, time.time()))\n            while True:\n                try:\n                    os.waitpid(child_pid, 0)\n                    break\n                except OSError as e:\n                    # In case we encountered an OSError due to EINTR (which is\n                    # caused by a SIGINT or SIGTERM signal during\n                    # os.waitpid()), we simply ignore it and enter the next\n                    # iteration of the loop, waiting for the child to end.  In\n                    # any other case, this is some other unexpected OS error,\n                    # which we don't want to catch, so we re-raise those ones.\n                    if e.errno != errno.EINTR:\n                        raise\n\n    def main_work_horse(self, job):\n        \"\"\"This is the entry point of the newly spawned work horse.\"\"\"\n        # After fork()'ing, always assure we are generating random sequences\n        # that are different from the worker.\n        random.seed()\n\n        # Always ignore Ctrl+C in the work horse, as it might abort the\n        # currently running job.\n        # The main worker catches the Ctrl+C and requests graceful shutdown\n        # after the current work is done.  When cold shutdown is requested, it\n        # kills the current job anyway.\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        signal.signal(signal.SIGTERM, signal.SIG_DFL)\n\n        self._is_horse = True\n        self.log = logger\n\n        success = self.perform_job(job)\n\n        # os._exit() is the way to exit from childs after a fork(), in\n        # constrast to the regular sys.exit()\n        os._exit(int(not success))\n\n    def perform_job(self, job):\n        \"\"\"Performs the actual work of a job.  Will/should only be called\n        inside the work horse's process.\n        \"\"\"\n\n        self.set_state('busy')\n        self.set_current_job_id(job.id)\n        self.heartbeat((job.timeout or 180) + 60)\n\n        self.procline('Processing %s from %s since %s' % (\n            job.func_name,\n            job.origin, time.time()))\n\n        with self.connection._pipeline() as pipeline:\n            try:\n                with self.death_penalty_class(job.timeout or self.queue_class.DEFAULT_TIMEOUT):\n                    rv = job.perform()\n\n                # Pickle the result in the same try-except block since we need to\n                # use the same exc handling when pickling fails\n                job._result = rv\n\n                self.set_current_job_id(None, pipeline=pipeline)\n\n                result_ttl = job.get_ttl(self.default_result_ttl)\n                if result_ttl != 0:\n                    job.save(pipeline=pipeline)\n                job.cleanup(result_ttl, pipeline=pipeline)\n\n                pipeline.execute()\n\n            except Exception:\n                # Use the public setter here, to immediately update Redis\n                job.set_status(Status.FAILED)\n                self.handle_exception(job, *sys.exc_info())\n                return False\n\n        if rv is None:\n            self.log.info('Job OK')\n        else:\n            self.log.info('Job OK, result = %s' % (yellow(text_type(rv)),))\n\n        if result_ttl == 0:\n            self.log.info('Result discarded immediately.')\n        elif result_ttl > 0:\n            self.log.info('Result is kept for %d seconds.' % result_ttl)\n        else:\n            self.log.warning('Result will never expire, clean up result key manually.')\n\n        return True\n\n    def handle_exception(self, job, *exc_info):\n        \"\"\"Walks the exception handler stack to delegate exception handling.\"\"\"\n        exc_string = ''.join(traceback.format_exception_only(*exc_info[:2]) +\n                             traceback.format_exception(*exc_info))\n        self.log.error(exc_string)\n\n        for handler in reversed(self._exc_handlers):\n            self.log.debug('Invoking exception handler %s' % (handler,))\n            fallthrough = handler(job, *exc_info)\n\n            # Only handlers with explicit return values should disable further\n            # exc handling, so interpret a None return value as True.\n            if fallthrough is None:\n                fallthrough = True\n\n            if not fallthrough:\n                break\n\n    def move_to_failed_queue(self, job, *exc_info):\n        \"\"\"Default exception handler: move the job to the failed queue.\"\"\"\n        exc_string = ''.join(traceback.format_exception(*exc_info))\n        self.log.warning('Moving job to %s queue.' % self.failed_queue.name)\n        self.failed_queue.quarantine(job, exc_info=exc_string)\n\n    def push_exc_handler(self, handler_func):\n        \"\"\"Pushes an exception handler onto the exc handler stack.\"\"\"\n        self._exc_handlers.append(handler_func)\n\n    def pop_exc_handler(self):\n        \"\"\"Pops the latest exception handler off of the exc handler stack.\"\"\"\n        return self._exc_handlers.pop()\n"
                }
            ]
        }, 
        {
            "type": "exec", 
            "name": "run_tests", 
            "contents": "#!/bin/sh", 
            "description": "check_redis_running() {"
        }, 
        {
            "type": "text", 
            "name": "setup.cfg", 
            "contents": "[bdist_rpm]\nrequires = redis\n\n[wheel]\nuniversal = 1\n"
        }, 
        {
            "type": "text", 
            "name": "setup.py", 
            "contents": "\"\"\"\nrq is a simple, lightweight, library for creating background jobs, and\nprocessing them.\n\"\"\"\nimport sys\nimport os\nfrom setuptools import setup, find_packages\n\n\ndef get_version():\n    basedir = os.path.dirname(__file__)\n    with open(os.path.join(basedir, 'rq/version.py')) as f:\n        locals = {}\n        exec(f.read(), locals)\n        return locals['VERSION']\n    raise RuntimeError('No version info found.')\n\n\ndef get_dependencies():\n    deps = ['redis >= 2.7.0']\n    if sys.version_info < (2, 7) or \\\n            (sys.version_info >= (3, 0) and sys.version_info < (3, 1)):\n        deps += ['importlib']\n    if sys.version_info < (2, 7) or \\\n            (sys.version_info >= (3, 0) and sys.version_info < (3, 2)):\n        deps += ['argparse']\n    return deps\n\nsetup(\n    name='rq',\n    version=get_version(),\n    url='https://github.com/nvie/rq/',\n    license='BSD',\n    author='Vincent Driessen',\n    author_email='vincent@3rdcloud.com',\n    description='RQ is a simple, lightweight, library for creating background '\n                'jobs, and processing them.',\n    long_description=__doc__,\n    packages=find_packages(exclude=['tests']),\n    include_package_data=True,\n    zip_safe=False,\n    platforms='any',\n    install_requires=get_dependencies(),\n    entry_points='''\\\n    [console_scripts]\n    rqworker = rq.scripts.rqworker:main\n    rqinfo = rq.scripts.rqinfo:main\n    ''',\n    classifiers=[\n        # As from http://pypi.python.org/pypi?%3Aaction=list_classifiers\n        #'Development Status :: 1 - Planning',\n        #'Development Status :: 2 - Pre-Alpha',\n        #'Development Status :: 3 - Alpha',\n        'Development Status :: 4 - Beta',\n        #'Development Status :: 5 - Production/Stable',\n        #'Development Status :: 6 - Mature',\n        #'Development Status :: 7 - Inactive',\n        'Intended Audience :: Developers',\n        'Intended Audience :: End Users/Desktop',\n        'Intended Audience :: Information Technology',\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: System Administrators',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: POSIX',\n        'Operating System :: MacOS',\n        'Operating System :: Unix',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 3',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n        'Topic :: Internet',\n        'Topic :: Scientific/Engineering',\n        'Topic :: System :: Distributed Computing',\n        'Topic :: System :: Systems Administration',\n        'Topic :: System :: Monitoring',\n\n    ]\n)\n"
        }, 
        {
            "type": "dir", 
            "name": "tests", 
            "contents": [
                {
                    "type": "text", 
                    "name": "__init__.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\n\nfrom redis import StrictRedis\nfrom rq import pop_connection, push_connection\nfrom rq.compat import is_python_version\n\nif is_python_version((2, 7), (3, 2)):\n    import unittest\nelse:\n    import unittest2 as unittest  # noqa\n\n\ndef find_empty_redis_database():\n    \"\"\"Tries to connect to a random Redis database (starting from 4), and\n    will use/connect it when no keys are in there.\n    \"\"\"\n    for dbnum in range(4, 17):\n        testconn = StrictRedis(db=dbnum)\n        empty = len(testconn.keys('*')) == 0\n        if empty:\n            return testconn\n    assert False, 'No empty Redis database found to run tests in.'\n\n\ndef slow(f):\n    import os\n    from functools import wraps\n\n    @wraps(f)\n    def _inner(*args, **kwargs):\n        if os.environ.get('ONLY_RUN_FAST_TESTS'):\n            f(*args, **kwargs)\n\n    return _inner\n\n\nclass RQTestCase(unittest.TestCase):\n    \"\"\"Base class to inherit test cases from for RQ.\n\n    It sets up the Redis connection (available via self.testconn), turns off\n    logging to the terminal and flushes the Redis database before and after\n    running each test.\n\n    Also offers assertQueueContains(queue, that_func) assertion method.\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        # Set up connection to Redis\n        testconn = find_empty_redis_database()\n        push_connection(testconn)\n\n        # Store the connection (for sanity checking)\n        cls.testconn = testconn\n\n        # Shut up logging\n        logging.disable(logging.ERROR)\n\n    def setUp(self):\n        # Flush beforewards (we like our hygiene)\n        self.testconn.flushdb()\n\n    def tearDown(self):\n        # Flush afterwards\n        self.testconn.flushdb()\n\n    # Implement assertIsNotNone for Python runtimes < 2.7 or < 3.1\n    if not hasattr(unittest.TestCase, 'assertIsNotNone'):\n        def assertIsNotNone(self, value, *args):\n            self.assertNotEqual(value, None, *args)\n\n    @classmethod\n    def tearDownClass(cls):\n        logging.disable(logging.NOTSET)\n\n        # Pop the connection to Redis\n        testconn = pop_connection()\n        assert testconn == cls.testconn, \\\n            'Wow, something really nasty happened to the Redis connection stack. Check your setup.'\n"
                }, 
                {
                    "type": "text", 
                    "name": "dummy_settings.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nREDIS_HOST = \"testhost.example.com\"\n"
                }, 
                {
                    "type": "text", 
                    "name": "fixtures.py", 
                    "contents": "# -*- coding: utf-8 -*-\n\"\"\"\nThis file contains all jobs that are used in tests.  Each of these test\nfixtures has a slighty different characteristics.\n\"\"\"\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport time\n\nfrom rq import Connection, get_current_job\nfrom rq.decorators import job\n\n\ndef say_hello(name=None):\n    \"\"\"A job with a single argument and a return value.\"\"\"\n    if name is None:\n        name = 'Stranger'\n    return 'Hi there, %s!' % (name,)\n\n\ndef do_nothing():\n    \"\"\"The best job in the world.\"\"\"\n    pass\n\n\ndef div_by_zero(x):\n    \"\"\"Prepare for a division-by-zero exception.\"\"\"\n    return x / 0\n\n\ndef some_calculation(x, y, z=1):\n    \"\"\"Some arbitrary calculation with three numbers.  Choose z smartly if you\n    want a division by zero exception.\n    \"\"\"\n    return x * y / z\n\n\ndef create_file(path):\n    \"\"\"Creates a file at the given path.  Actually, leaves evidence that the\n    job ran.\"\"\"\n    with open(path, 'w') as f:\n        f.write('Just a sentinel.')\n\n\ndef create_file_after_timeout(path, timeout):\n    time.sleep(timeout)\n    create_file(path)\n\n\ndef access_self():\n    job = get_current_job()\n    return job.id\n\n\ndef echo(*args, **kwargs):\n    return (args, kwargs)\n\n\nclass Number(object):\n    def __init__(self, value):\n        self.value = value\n\n    @classmethod\n    def divide(cls, x, y):\n        return x * y\n\n    def div(self, y):\n        return self.value / y\n\n\nwith Connection():\n    @job(queue='default')\n    def decorated_job(x, y):\n        return x + y\n\n\ndef long_running_job():\n    time.sleep(10)\n"
                }, 
                {
                    "type": "text", 
                    "name": "helpers.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom datetime import timedelta\n\n\ndef strip_microseconds(date):\n    return date - timedelta(microseconds=date.microsecond)\n"
                }, 
                {
                    "type": "text", 
                    "name": "test_connection.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom rq import Connection, Queue\n\nfrom tests import find_empty_redis_database, RQTestCase\nfrom tests.fixtures import do_nothing\n\n\ndef new_connection():\n    return find_empty_redis_database()\n\n\nclass TestConnectionInheritance(RQTestCase):\n    def test_connection_detection(self):\n        \"\"\"Automatic detection of the connection.\"\"\"\n        q = Queue()\n        self.assertEquals(q.connection, self.testconn)\n\n    def test_connection_stacking(self):\n        \"\"\"Connection stacking.\"\"\"\n        conn1 = new_connection()\n        conn2 = new_connection()\n\n        with Connection(conn1):\n            q1 = Queue()\n            with Connection(conn2):\n                q2 = Queue()\n        self.assertNotEquals(q1.connection, q2.connection)\n\n    def test_connection_pass_thru(self):\n        \"\"\"Connection passed through from queues to jobs.\"\"\"\n        q1 = Queue()\n        with Connection(new_connection()):\n            q2 = Queue()\n        job1 = q1.enqueue(do_nothing)\n        job2 = q2.enqueue(do_nothing)\n        self.assertEquals(q1.connection, job1.connection)\n        self.assertEquals(q2.connection, job2.connection)\n"
                }, 
                {
                    "type": "text", 
                    "name": "test_decorator.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport mock\nfrom redis import StrictRedis\nfrom rq.decorators import job\nfrom rq.job import Job\nfrom rq.worker import DEFAULT_RESULT_TTL\n\nfrom tests import RQTestCase\nfrom tests.fixtures import decorated_job\n\n\nclass TestDecorator(RQTestCase):\n\n    def setUp(self):\n        super(TestDecorator, self).setUp()\n\n    def test_decorator_preserves_functionality(self):\n        \"\"\"Ensure that a decorated function's functionality is still preserved.\n        \"\"\"\n        self.assertEqual(decorated_job(1, 2), 3)\n\n    def test_decorator_adds_delay_attr(self):\n        \"\"\"Ensure that decorator adds a delay attribute to function that returns\n        a Job instance when called.\n        \"\"\"\n        self.assertTrue(hasattr(decorated_job, 'delay'))\n        result = decorated_job.delay(1, 2)\n        self.assertTrue(isinstance(result, Job))\n        # Ensure that job returns the right result when performed\n        self.assertEqual(result.perform(), 3)\n\n    def test_decorator_accepts_queue_name_as_argument(self):\n        \"\"\"Ensure that passing in queue name to the decorator puts the job in\n        the right queue.\n        \"\"\"\n        @job(queue='queue_name')\n        def hello():\n            return 'Hi'\n        result = hello.delay()\n        self.assertEqual(result.origin, 'queue_name')\n\n    def test_decorator_accepts_result_ttl_as_argument(self):\n        \"\"\"Ensure that passing in result_ttl to the decorator sets the\n        result_ttl on the job\n        \"\"\"\n        # Ensure default\n        result = decorated_job.delay(1, 2)\n        self.assertEqual(result.result_ttl, DEFAULT_RESULT_TTL)\n\n        @job('default', result_ttl=10)\n        def hello():\n            return 'Why hello'\n        result = hello.delay()\n        self.assertEqual(result.result_ttl, 10)\n\n    def test_decorator_accepts_result_depends_on_as_argument(self):\n        \"\"\"Ensure that passing in depends_on to the decorator sets the\n        correct dependency on the job\n        \"\"\"\n\n        @job(queue='queue_name')\n        def foo():\n            return 'Firstly'\n\n        @job(queue='queue_name')\n        def bar():\n            return 'Secondly'\n\n        foo_job = foo.delay()\n        bar_job = bar.delay(depends_on=foo_job)\n\n        self.assertIsNone(foo_job._dependency_id)\n\n        self.assertEqual(bar_job.dependency, foo_job)\n\n        self.assertEqual(bar_job._dependency_id, foo_job.id)\n\n    @mock.patch('rq.queue.resolve_connection')\n    def test_decorator_connection_laziness(self, resolve_connection):\n        \"\"\"Ensure that job decorator resolve connection in `lazy` way \"\"\"\n\n        resolve_connection.return_value = StrictRedis()\n\n        @job(queue='queue_name')\n        def foo():\n            return 'do something'\n\n        self.assertEqual(resolve_connection.call_count, 0)\n\n        foo()\n\n        self.assertEqual(resolve_connection.call_count, 0)\n\n        foo.delay()\n\n        self.assertEqual(resolve_connection.call_count, 1)\n"
                }, 
                {
                    "type": "text", 
                    "name": "test_job.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom datetime import datetime\n\nfrom rq.compat import as_text, PY2\nfrom rq.exceptions import NoSuchJobError, UnpickleError\nfrom rq.job import get_current_job, Job\nfrom rq.queue import Queue\nfrom rq.utils import utcformat\n\nfrom tests import RQTestCase\nfrom tests.fixtures import access_self, Number, say_hello, some_calculation\nfrom tests.helpers import strip_microseconds\n\ntry:\n    from cPickle import loads, dumps\nexcept ImportError:\n    from pickle import loads, dumps\n\n\nclass TestJob(RQTestCase):\n    def test_create_empty_job(self):\n        \"\"\"Creation of new empty jobs.\"\"\"\n        job = Job()\n\n        # Jobs have a random UUID and a creation date\n        self.assertIsNotNone(job.id)\n        self.assertIsNotNone(job.created_at)\n\n        # ...and nothing else\n        self.assertIsNone(job.origin)\n        self.assertIsNone(job.enqueued_at)\n        self.assertIsNone(job.ended_at)\n        self.assertIsNone(job.result)\n        self.assertIsNone(job.exc_info)\n\n        with self.assertRaises(ValueError):\n            job.func\n        with self.assertRaises(ValueError):\n            job.instance\n        with self.assertRaises(ValueError):\n            job.args\n        with self.assertRaises(ValueError):\n            job.kwargs\n\n    def test_create_typical_job(self):\n        \"\"\"Creation of jobs for function calls.\"\"\"\n        job = Job.create(func=some_calculation, args=(3, 4), kwargs=dict(z=2))\n\n        # Jobs have a random UUID\n        self.assertIsNotNone(job.id)\n        self.assertIsNotNone(job.created_at)\n        self.assertIsNotNone(job.description)\n        self.assertIsNone(job.instance)\n\n        # Job data is set...\n        self.assertEquals(job.func, some_calculation)\n        self.assertEquals(job.args, (3, 4))\n        self.assertEquals(job.kwargs, {'z': 2})\n\n        # ...but metadata is not\n        self.assertIsNone(job.origin)\n        self.assertIsNone(job.enqueued_at)\n        self.assertIsNone(job.result)\n\n    def test_create_instance_method_job(self):\n        \"\"\"Creation of jobs for instance methods.\"\"\"\n        n = Number(2)\n        job = Job.create(func=n.div, args=(4,))\n\n        # Job data is set\n        self.assertEquals(job.func, n.div)\n        self.assertEquals(job.instance, n)\n        self.assertEquals(job.args, (4,))\n\n    def test_create_job_from_string_function(self):\n        \"\"\"Creation of jobs using string specifier.\"\"\"\n        job = Job.create(func='tests.fixtures.say_hello', args=('World',))\n\n        # Job data is set\n        self.assertEquals(job.func, say_hello)\n        self.assertIsNone(job.instance)\n        self.assertEquals(job.args, ('World',))\n\n    def test_job_properties_set_data_property(self):\n        \"\"\"Data property gets derived from the job tuple.\"\"\"\n        job = Job()\n        job.func_name = 'foo'\n        fname, instance, args, kwargs = loads(job.data)\n\n        self.assertEquals(fname, job.func_name)\n        self.assertEquals(instance, None)\n        self.assertEquals(args, ())\n        self.assertEquals(kwargs, {})\n\n    def test_data_property_sets_job_properties(self):\n        \"\"\"Job tuple gets derived lazily from data property.\"\"\"\n        job = Job()\n        job.data = dumps(('foo', None, (1, 2, 3), {'bar': 'qux'}))\n\n        self.assertEquals(job.func_name, 'foo')\n        self.assertEquals(job.instance, None)\n        self.assertEquals(job.args, (1, 2, 3))\n        self.assertEquals(job.kwargs, {'bar': 'qux'})\n\n    def test_save(self):  # noqa\n        \"\"\"Storing jobs.\"\"\"\n        job = Job.create(func=some_calculation, args=(3, 4), kwargs=dict(z=2))\n\n        # Saving creates a Redis hash\n        self.assertEquals(self.testconn.exists(job.key), False)\n        job.save()\n        self.assertEquals(self.testconn.type(job.key), b'hash')\n\n        # Saving writes pickled job data\n        unpickled_data = loads(self.testconn.hget(job.key, 'data'))\n        self.assertEquals(unpickled_data[0], 'tests.fixtures.some_calculation')\n\n    def test_fetch(self):\n        \"\"\"Fetching jobs.\"\"\"\n        # Prepare test\n        self.testconn.hset('rq:job:some_id', 'data',\n                           \"(S'tests.fixtures.some_calculation'\\nN(I3\\nI4\\nt(dp1\\nS'z'\\nI2\\nstp2\\n.\")\n        self.testconn.hset('rq:job:some_id', 'created_at',\n                           '2012-02-07T22:13:24Z')\n\n        # Fetch returns a job\n        job = Job.fetch('some_id')\n        self.assertEquals(job.id, 'some_id')\n        self.assertEquals(job.func_name, 'tests.fixtures.some_calculation')\n        self.assertIsNone(job.instance)\n        self.assertEquals(job.args, (3, 4))\n        self.assertEquals(job.kwargs, dict(z=2))\n        self.assertEquals(job.created_at, datetime(2012, 2, 7, 22, 13, 24))\n\n    def test_persistence_of_empty_jobs(self):  # noqa\n        \"\"\"Storing empty jobs.\"\"\"\n        job = Job()\n        with self.assertRaises(ValueError):\n            job.save()\n\n    def test_persistence_of_typical_jobs(self):\n        \"\"\"Storing typical jobs.\"\"\"\n        job = Job.create(func=some_calculation, args=(3, 4), kwargs=dict(z=2))\n        job.save()\n\n        expected_date = strip_microseconds(job.created_at)\n        stored_date = self.testconn.hget(job.key, 'created_at').decode('utf-8')\n        self.assertEquals(\n            stored_date,\n            utcformat(expected_date))\n\n        # ... and no other keys are stored\n        self.assertEqual(\n            sorted(self.testconn.hkeys(job.key)),\n            [b'created_at', b'data', b'description'])\n\n    def test_persistence_of_parent_job(self):\n        \"\"\"Storing jobs with parent job, either instance or key.\"\"\"\n        parent_job = Job.create(func=some_calculation)\n        parent_job.save()\n        job = Job.create(func=some_calculation, depends_on=parent_job)\n        job.save()\n        stored_job = Job.fetch(job.id)\n        self.assertEqual(stored_job._dependency_id, parent_job.id)\n        self.assertEqual(stored_job.dependency, parent_job)\n\n        job = Job.create(func=some_calculation, depends_on=parent_job.id)\n        job.save()\n        stored_job = Job.fetch(job.id)\n        self.assertEqual(stored_job._dependency_id, parent_job.id)\n        self.assertEqual(stored_job.dependency, parent_job)\n\n    def test_store_then_fetch(self):\n        \"\"\"Store, then fetch.\"\"\"\n        job = Job.create(func=some_calculation, args=(3, 4), kwargs=dict(z=2))\n        job.save()\n\n        job2 = Job.fetch(job.id)\n        self.assertEquals(job.func, job2.func)\n        self.assertEquals(job.args, job2.args)\n        self.assertEquals(job.kwargs, job2.kwargs)\n\n        # Mathematical equation\n        self.assertEquals(job, job2)\n\n    def test_fetching_can_fail(self):\n        \"\"\"Fetching fails for non-existing jobs.\"\"\"\n        with self.assertRaises(NoSuchJobError):\n            Job.fetch('b4a44d44-da16-4620-90a6-798e8cd72ca0')\n\n    def test_fetching_unreadable_data(self):\n        \"\"\"Fetching succeeds on unreadable data, but lazy props fail.\"\"\"\n        # Set up\n        job = Job.create(func=some_calculation, args=(3, 4), kwargs=dict(z=2))\n        job.save()\n\n        # Just replace the data hkey with some random noise\n        self.testconn.hset(job.key, 'data', 'this is no pickle string')\n        job.refresh()\n\n        for attr in ('func_name', 'instance', 'args', 'kwargs'):\n            with self.assertRaises(UnpickleError):\n                getattr(job, attr)\n\n    def test_job_is_unimportable(self):\n        \"\"\"Jobs that cannot be imported throw exception on access.\"\"\"\n        job = Job.create(func=say_hello, args=('Lionel',))\n        job.save()\n\n        # Now slightly modify the job to make it unimportable (this is\n        # equivalent to a worker not having the most up-to-date source code\n        # and unable to import the function)\n        data = self.testconn.hget(job.key, 'data')\n        unimportable_data = data.replace(b'say_hello', b'shut_up')\n        self.testconn.hset(job.key, 'data', unimportable_data)\n\n        job.refresh()\n        with self.assertRaises(AttributeError):\n            job.func  # accessing the func property should fail\n\n    def test_custom_meta_is_persisted(self):\n        \"\"\"Additional meta data on jobs are stored persisted correctly.\"\"\"\n        job = Job.create(func=say_hello, args=('Lionel',))\n        job.meta['foo'] = 'bar'\n        job.save()\n\n        raw_data = self.testconn.hget(job.key, 'meta')\n        self.assertEqual(loads(raw_data)['foo'], 'bar')\n\n        job2 = Job.fetch(job.id)\n        self.assertEqual(job2.meta['foo'], 'bar')\n\n    def test_result_ttl_is_persisted(self):\n        \"\"\"Ensure that job's result_ttl is set properly\"\"\"\n        job = Job.create(func=say_hello, args=('Lionel',), result_ttl=10)\n        job.save()\n        Job.fetch(job.id, connection=self.testconn)\n        self.assertEqual(job.result_ttl, 10)\n\n        job = Job.create(func=say_hello, args=('Lionel',))\n        job.save()\n        Job.fetch(job.id, connection=self.testconn)\n        self.assertEqual(job.result_ttl, None)\n\n    def test_description_is_persisted(self):\n        \"\"\"Ensure that job's custom description is set properly\"\"\"\n        job = Job.create(func=say_hello, args=('Lionel',), description='Say hello!')\n        job.save()\n        Job.fetch(job.id, connection=self.testconn)\n        self.assertEqual(job.description, 'Say hello!')\n\n        # Ensure job description is constructed from function call string\n        job = Job.create(func=say_hello, args=('Lionel',))\n        job.save()\n        Job.fetch(job.id, connection=self.testconn)\n        if PY2:\n            self.assertEqual(job.description, \"tests.fixtures.say_hello(u'Lionel')\")\n        else:\n            self.assertEqual(job.description, \"tests.fixtures.say_hello('Lionel')\")\n\n    def test_job_access_within_job_function(self):\n        \"\"\"The current job is accessible within the job function.\"\"\"\n        # Executing the job function from outside of RQ throws an exception\n        self.assertIsNone(get_current_job())\n\n        # Executing the job function from within the job works (and in\n        # this case leads to the job ID being returned)\n        job = Job.create(func=access_self)\n        job.save()\n        id = job.perform()\n        self.assertEqual(job.id, id)\n        self.assertEqual(job.func, access_self)\n\n        # Ensure that get_current_job also works from within synchronous jobs\n        queue = Queue(async=False)\n        job = queue.enqueue(access_self)\n        id = job.perform()\n        self.assertEqual(job.id, id)\n        self.assertEqual(job.func, access_self)\n\n    def test_get_ttl(self):\n        \"\"\"Getting job TTL.\"\"\"\n        job_ttl = 1\n        default_ttl = 2\n        job = Job.create(func=say_hello, result_ttl=job_ttl)\n        job.save()\n        self.assertEqual(job.get_ttl(default_ttl=default_ttl), job_ttl)\n        self.assertEqual(job.get_ttl(), job_ttl)\n        job = Job.create(func=say_hello)\n        job.save()\n        self.assertEqual(job.get_ttl(default_ttl=default_ttl), default_ttl)\n        self.assertEqual(job.get_ttl(), None)\n\n    def test_cleanup(self):\n        \"\"\"Test that jobs and results are expired properly.\"\"\"\n        job = Job.create(func=say_hello)\n        job.save()\n\n        # Jobs with negative TTLs don't expire\n        job.cleanup(ttl=-1)\n        self.assertEqual(self.testconn.ttl(job.key), -1)\n\n        # Jobs with positive TTLs are eventually deleted\n        job.cleanup(ttl=100)\n        self.assertEqual(self.testconn.ttl(job.key), 100)\n\n        # Jobs with 0 TTL are immediately deleted\n        job.cleanup(ttl=0)\n        self.assertRaises(NoSuchJobError, Job.fetch, job.id, self.testconn)\n\n    def test_register_dependency(self):\n        \"\"\"Test that jobs updates the correct job dependents.\"\"\"\n        job = Job.create(func=say_hello)\n        job._dependency_id = 'id'\n        job.save()\n        job.register_dependency()\n        self.assertEqual(as_text(self.testconn.spop('rq:job:id:dependents')), job.id)\n\n    def test_cancel(self):\n        \"\"\"job.cancel() deletes itself & dependents mapping from Redis.\"\"\"\n        job = Job.create(func=say_hello)\n        job2 = Job.create(func=say_hello, depends_on=job)\n        job2.register_dependency()\n        job.cancel()\n        self.assertFalse(self.testconn.exists(job.key))\n        self.assertFalse(self.testconn.exists(job.dependents_key))\n"
                }, 
                {
                    "type": "text", 
                    "name": "test_queue.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom rq import get_failed_queue, Queue\nfrom rq.exceptions import InvalidJobOperationError\nfrom rq.job import Job, Status\nfrom rq.worker import Worker\n\nfrom tests import RQTestCase\nfrom tests.fixtures import (div_by_zero, echo, Number, say_hello,\n                            some_calculation)\n\n\nclass TestQueue(RQTestCase):\n    def test_create_queue(self):\n        \"\"\"Creating queues.\"\"\"\n        q = Queue('my-queue')\n        self.assertEquals(q.name, 'my-queue')\n\n    def test_create_default_queue(self):\n        \"\"\"Instantiating the default queue.\"\"\"\n        q = Queue()\n        self.assertEquals(q.name, 'default')\n\n    def test_equality(self):\n        \"\"\"Mathematical equality of queues.\"\"\"\n        q1 = Queue('foo')\n        q2 = Queue('foo')\n        q3 = Queue('bar')\n\n        self.assertEquals(q1, q2)\n        self.assertEquals(q2, q1)\n        self.assertNotEquals(q1, q3)\n        self.assertNotEquals(q2, q3)\n\n    def test_empty_queue(self):\n        \"\"\"Emptying queues.\"\"\"\n        q = Queue('example')\n\n        self.testconn.rpush('rq:queue:example', 'foo')\n        self.testconn.rpush('rq:queue:example', 'bar')\n        self.assertEquals(q.is_empty(), False)\n\n        q.empty()\n\n        self.assertEquals(q.is_empty(), True)\n        self.assertIsNone(self.testconn.lpop('rq:queue:example'))\n\n    def test_empty_removes_jobs(self):\n        \"\"\"Emptying a queue deletes the associated job objects\"\"\"\n        q = Queue('example')\n        job = q.enqueue(say_hello)\n        self.assertTrue(Job.exists(job.id))\n        q.empty()\n        self.assertFalse(Job.exists(job.id))\n\n    def test_queue_is_empty(self):\n        \"\"\"Detecting empty queues.\"\"\"\n        q = Queue('example')\n        self.assertEquals(q.is_empty(), True)\n\n        self.testconn.rpush('rq:queue:example', 'sentinel message')\n        self.assertEquals(q.is_empty(), False)\n\n    def test_remove(self):\n        \"\"\"Ensure queue.remove properly removes Job from queue.\"\"\"\n        q = Queue('example')\n        job = q.enqueue(say_hello)\n        self.assertIn(job.id, q.job_ids)\n        q.remove(job)\n        self.assertNotIn(job.id, q.job_ids)\n\n        job = q.enqueue(say_hello)\n        self.assertIn(job.id, q.job_ids)\n        q.remove(job.id)\n        self.assertNotIn(job.id, q.job_ids)\n\n    def test_jobs(self):\n        \"\"\"Getting jobs out of a queue.\"\"\"\n        q = Queue('example')\n        self.assertEqual(q.jobs, [])\n        job = q.enqueue(say_hello)\n        self.assertEqual(q.jobs, [job])\n\n        # Fetching a deleted removes it from queue\n        job.delete()\n        self.assertEqual(q.job_ids, [job.id])\n        q.jobs\n        self.assertEqual(q.job_ids, [])\n\n    def test_compact(self):\n        \"\"\"Compacting queueus.\"\"\"\n        q = Queue()\n\n        q.enqueue(say_hello, 'Alice')\n        bob = q.enqueue(say_hello, 'Bob')\n        q.enqueue(say_hello, 'Charlie')\n        debrah = q.enqueue(say_hello, 'Debrah')\n\n        bob.cancel()\n        debrah.cancel()\n\n        self.assertEquals(q.count, 4)\n\n        q.compact()\n\n        self.assertEquals(q.count, 2)\n\n    def test_enqueue(self):\n        \"\"\"Enqueueing job onto queues.\"\"\"\n        q = Queue()\n        self.assertEquals(q.is_empty(), True)\n\n        # say_hello spec holds which queue this is sent to\n        job = q.enqueue(say_hello, 'Nick', foo='bar')\n        job_id = job.id\n\n        # Inspect data inside Redis\n        q_key = 'rq:queue:default'\n        self.assertEquals(self.testconn.llen(q_key), 1)\n        self.assertEquals(\n            self.testconn.lrange(q_key, 0, -1)[0].decode('ascii'),\n            job_id)\n\n    def test_enqueue_sets_metadata(self):\n        \"\"\"Enqueueing job onto queues modifies meta data.\"\"\"\n        q = Queue()\n        job = Job.create(func=say_hello, args=('Nick',), kwargs=dict(foo='bar'))\n\n        # Preconditions\n        self.assertIsNone(job.origin)\n        self.assertIsNone(job.enqueued_at)\n\n        # Action\n        q.enqueue_job(job)\n\n        # Postconditions\n        self.assertEquals(job.origin, q.name)\n        self.assertIsNotNone(job.enqueued_at)\n\n    def test_pop_job_id(self):\n        \"\"\"Popping job IDs from queues.\"\"\"\n        # Set up\n        q = Queue()\n        uuid = '112188ae-4e9d-4a5b-a5b3-f26f2cb054da'\n        q.push_job_id(uuid)\n\n        # Pop it off the queue...\n        self.assertEquals(q.count, 1)\n        self.assertEquals(q.pop_job_id(), uuid)\n\n        # ...and assert the queue count when down\n        self.assertEquals(q.count, 0)\n\n    def test_dequeue(self):\n        \"\"\"Dequeueing jobs from queues.\"\"\"\n        # Set up\n        q = Queue()\n        result = q.enqueue(say_hello, 'Rick', foo='bar')\n\n        # Dequeue a job (not a job ID) off the queue\n        self.assertEquals(q.count, 1)\n        job = q.dequeue()\n        self.assertEquals(job.id, result.id)\n        self.assertEquals(job.func, say_hello)\n        self.assertEquals(job.origin, q.name)\n        self.assertEquals(job.args[0], 'Rick')\n        self.assertEquals(job.kwargs['foo'], 'bar')\n\n        # ...and assert the queue count when down\n        self.assertEquals(q.count, 0)\n\n    def test_dequeue_instance_method(self):\n        \"\"\"Dequeueing instance method jobs from queues.\"\"\"\n        q = Queue()\n        n = Number(2)\n        q.enqueue(n.div, 4)\n\n        job = q.dequeue()\n\n        # The instance has been pickled and unpickled, so it is now a separate\n        # object. Test for equality using each object's __dict__ instead.\n        self.assertEquals(job.instance.__dict__, n.__dict__)\n        self.assertEquals(job.func.__name__, 'div')\n        self.assertEquals(job.args, (4,))\n\n    def test_dequeue_class_method(self):\n        \"\"\"Dequeueing class method jobs from queues.\"\"\"\n        q = Queue()\n        q.enqueue(Number.divide, 3, 4)\n\n        job = q.dequeue()\n\n        self.assertEquals(job.instance.__dict__, Number.__dict__)\n        self.assertEquals(job.func.__name__, 'divide')\n        self.assertEquals(job.args, (3, 4))\n\n    def test_dequeue_ignores_nonexisting_jobs(self):\n        \"\"\"Dequeuing silently ignores non-existing jobs.\"\"\"\n\n        q = Queue()\n        uuid = '49f205ab-8ea3-47dd-a1b5-bfa186870fc8'\n        q.push_job_id(uuid)\n        q.push_job_id(uuid)\n        result = q.enqueue(say_hello, 'Nick', foo='bar')\n        q.push_job_id(uuid)\n\n        # Dequeue simply ignores the missing job and returns None\n        self.assertEquals(q.count, 4)\n        self.assertEquals(q.dequeue().id, result.id)\n        self.assertIsNone(q.dequeue())\n        self.assertEquals(q.count, 0)\n\n    def test_dequeue_any(self):\n        \"\"\"Fetching work from any given queue.\"\"\"\n        fooq = Queue('foo')\n        barq = Queue('bar')\n\n        self.assertEquals(Queue.dequeue_any([fooq, barq], None), None)\n\n        # Enqueue a single item\n        barq.enqueue(say_hello)\n        job, queue = Queue.dequeue_any([fooq, barq], None)\n        self.assertEquals(job.func, say_hello)\n        self.assertEquals(queue, barq)\n\n        # Enqueue items on both queues\n        barq.enqueue(say_hello, 'for Bar')\n        fooq.enqueue(say_hello, 'for Foo')\n\n        job, queue = Queue.dequeue_any([fooq, barq], None)\n        self.assertEquals(queue, fooq)\n        self.assertEquals(job.func, say_hello)\n        self.assertEquals(job.origin, fooq.name)\n        self.assertEquals(job.args[0], 'for Foo',\n                          'Foo should be dequeued first.')\n\n        job, queue = Queue.dequeue_any([fooq, barq], None)\n        self.assertEquals(queue, barq)\n        self.assertEquals(job.func, say_hello)\n        self.assertEquals(job.origin, barq.name)\n        self.assertEquals(job.args[0], 'for Bar',\n                          'Bar should be dequeued second.')\n\n    def test_dequeue_any_ignores_nonexisting_jobs(self):\n        \"\"\"Dequeuing (from any queue) silently ignores non-existing jobs.\"\"\"\n\n        q = Queue('low')\n        uuid = '49f205ab-8ea3-47dd-a1b5-bfa186870fc8'\n        q.push_job_id(uuid)\n\n        # Dequeue simply ignores the missing job and returns None\n        self.assertEquals(q.count, 1)\n        self.assertEquals(Queue.dequeue_any([Queue(), Queue('low')], None),  # noqa\n                None)\n        self.assertEquals(q.count, 0)\n\n    def test_enqueue_sets_status(self):\n        \"\"\"Enqueueing a job sets its status to \"queued\".\"\"\"\n        q = Queue()\n        job = q.enqueue(say_hello)\n        self.assertEqual(job.get_status(), Status.QUEUED)\n\n    def test_enqueue_explicit_args(self):\n        \"\"\"enqueue() works for both implicit/explicit args.\"\"\"\n        q = Queue()\n\n        # Implicit args/kwargs mode\n        job = q.enqueue(echo, 1, timeout=1, result_ttl=1, bar='baz')\n        self.assertEqual(job.timeout, 1)\n        self.assertEqual(job.result_ttl, 1)\n        self.assertEqual(\n            job.perform(),\n            ((1,), {'bar': 'baz'})\n        )\n\n        # Explicit kwargs mode\n        kwargs = {\n            'timeout': 1,\n            'result_ttl': 1,\n        }\n        job = q.enqueue(echo, timeout=2, result_ttl=2, args=[1], kwargs=kwargs)\n        self.assertEqual(job.timeout, 2)\n        self.assertEqual(job.result_ttl, 2)\n        self.assertEqual(\n            job.perform(),\n            ((1,), {'timeout': 1, 'result_ttl': 1})\n        )\n\n    def test_all_queues(self):\n        \"\"\"All queues\"\"\"\n        q1 = Queue('first-queue')\n        q2 = Queue('second-queue')\n        q3 = Queue('third-queue')\n\n        # Ensure a queue is added only once a job is enqueued\n        self.assertEquals(len(Queue.all()), 0)\n        q1.enqueue(say_hello)\n        self.assertEquals(len(Queue.all()), 1)\n\n        # Ensure this holds true for multiple queues\n        q2.enqueue(say_hello)\n        q3.enqueue(say_hello)\n        names = [q.name for q in Queue.all()]\n        self.assertEquals(len(Queue.all()), 3)\n\n        # Verify names\n        self.assertTrue('first-queue' in names)\n        self.assertTrue('second-queue' in names)\n        self.assertTrue('third-queue' in names)\n\n        # Now empty two queues\n        w = Worker([q2, q3])\n        w.work(burst=True)\n\n        # Queue.all() should still report the empty queues\n        self.assertEquals(len(Queue.all()), 3)\n\n    def test_enqueue_dependents(self):\n        \"\"\"Enqueueing the dependent jobs pushes all jobs in the depends set to the queue.\"\"\"\n        q = Queue()\n        parent_job = Job.create(func=say_hello)\n        parent_job.save()\n        job_1 = Job.create(func=say_hello, depends_on=parent_job)\n        job_1.save()\n        job_1.register_dependency()\n        job_2 = Job.create(func=say_hello, depends_on=parent_job)\n        job_2.save()\n        job_2.register_dependency()\n\n        # After dependents is enqueued, job_1 and job_2 should be in queue\n        self.assertEqual(q.job_ids, [])\n        q.enqueue_dependents(parent_job)\n        self.assertEqual(set(q.job_ids), set([job_1.id, job_2.id]))\n        self.assertFalse(self.testconn.exists(parent_job.dependents_key))\n\n    def test_enqueue_job_with_dependency(self):\n        \"\"\"Jobs are enqueued only when their dependencies are finished.\"\"\"\n        # Job with unfinished dependency is not immediately enqueued\n        parent_job = Job.create(func=say_hello)\n        q = Queue()\n        q.enqueue_call(say_hello, depends_on=parent_job)\n        self.assertEqual(q.job_ids, [])\n\n        # Jobs dependent on finished jobs are immediately enqueued\n        parent_job.set_status(Status.FINISHED)\n        parent_job.save()\n        job = q.enqueue_call(say_hello, depends_on=parent_job)\n        self.assertEqual(q.job_ids, [job.id])\n        self.assertEqual(job.timeout, Queue.DEFAULT_TIMEOUT)\n\n    def test_enqueue_job_with_dependency_and_timeout(self):\n        \"\"\"Jobs still know their specified timeout after being scheduled as a dependency.\"\"\"\n        # Job with unfinished dependency is not immediately enqueued\n        parent_job = Job.create(func=say_hello)\n        q = Queue()\n        job = q.enqueue_call(say_hello, depends_on=parent_job, timeout=123)\n        self.assertEqual(q.job_ids, [])\n        self.assertEqual(job.timeout, 123)\n\n        # Jobs dependent on finished jobs are immediately enqueued\n        parent_job.set_status(Status.FINISHED)\n        parent_job.save()\n        job = q.enqueue_call(say_hello, depends_on=parent_job, timeout=123)\n        self.assertEqual(q.job_ids, [job.id])\n        self.assertEqual(job.timeout, 123)\n\n\nclass TestFailedQueue(RQTestCase):\n    def test_requeue_job(self):\n        \"\"\"Requeueing existing jobs.\"\"\"\n        job = Job.create(func=div_by_zero, args=(1, 2, 3))\n        job.origin = 'fake'\n        job.save()\n        get_failed_queue().quarantine(job, Exception('Some fake error'))  # noqa\n\n        self.assertEqual(Queue.all(), [get_failed_queue()])  # noqa\n        self.assertEquals(get_failed_queue().count, 1)\n\n        get_failed_queue().requeue(job.id)\n\n        self.assertEquals(get_failed_queue().count, 0)\n        self.assertEquals(Queue('fake').count, 1)\n\n    def test_requeue_nonfailed_job_fails(self):\n        \"\"\"Requeueing non-failed jobs raises error.\"\"\"\n        q = Queue()\n        job = q.enqueue(say_hello, 'Nick', foo='bar')\n\n        # Assert that we cannot requeue a job that's not on the failed queue\n        with self.assertRaises(InvalidJobOperationError):\n            get_failed_queue().requeue(job.id)\n\n    def test_quarantine_preserves_timeout(self):\n        \"\"\"Quarantine preserves job timeout.\"\"\"\n        job = Job.create(func=div_by_zero, args=(1, 2, 3))\n        job.origin = 'fake'\n        job.timeout = 200\n        job.save()\n        get_failed_queue().quarantine(job, Exception('Some fake error'))\n\n        self.assertEquals(job.timeout, 200)\n\n    def test_requeueing_preserves_timeout(self):\n        \"\"\"Requeueing preserves job timeout.\"\"\"\n        job = Job.create(func=div_by_zero, args=(1, 2, 3))\n        job.origin = 'fake'\n        job.timeout = 200\n        job.save()\n        get_failed_queue().quarantine(job, Exception('Some fake error'))\n        get_failed_queue().requeue(job.id)\n\n        job = Job.fetch(job.id)\n        self.assertEquals(job.timeout, 200)\n\n    def test_requeue_sets_status_to_queued(self):\n        \"\"\"Requeueing a job should set its status back to QUEUED.\"\"\"\n        job = Job.create(func=div_by_zero, args=(1, 2, 3))\n        job.save()\n        get_failed_queue().quarantine(job, Exception('Some fake error'))\n        get_failed_queue().requeue(job.id)\n\n        job = Job.fetch(job.id)\n        self.assertEqual(job.get_status(), Status.QUEUED)\n\n    def test_enqueue_preserves_result_ttl(self):\n        \"\"\"Enqueueing persists result_ttl.\"\"\"\n        q = Queue()\n        job = q.enqueue(div_by_zero, args=(1, 2, 3), result_ttl=10)\n        self.assertEqual(job.result_ttl, 10)\n        job_from_queue = Job.fetch(job.id, connection=self.testconn)\n        self.assertEqual(int(job_from_queue.result_ttl), 10)\n\n    def test_async_false(self):\n        \"\"\"Executes a job immediately if async=False.\"\"\"\n        q = Queue(async=False)\n        job = q.enqueue(some_calculation, args=(2, 3))\n        self.assertEqual(job.return_value, 6)\n"
                }, 
                {
                    "type": "text", 
                    "name": "test_scripts.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom rq.compat import is_python_version\nfrom rq.scripts import read_config_file\n\nif is_python_version((2, 7), (3, 2)):\n    from unittest import TestCase\nelse:\n    from unittest2 import TestCase  # noqa\n\n\nclass TestScripts(TestCase):\n    def test_config_file(self):\n        settings = read_config_file(\"tests.dummy_settings\")\n        self.assertIn(\"REDIS_HOST\", settings)\n        self.assertEqual(settings['REDIS_HOST'], \"testhost.example.com\")\n"
                }, 
                {
                    "type": "text", 
                    "name": "test_sentry.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom rq import get_failed_queue, Queue, Worker\nfrom rq.contrib.sentry import register_sentry\n\nfrom tests import RQTestCase\n\n\nclass FakeSentry(object):\n    servers = []\n\n    def captureException(self, *args, **kwds):\n        pass  # we cannot check this, because worker forks\n\n\nclass TestSentry(RQTestCase):\n\n    def test_work_fails(self):\n        \"\"\"Non importable jobs should be put on the failed queue event with sentry\"\"\"\n        q = Queue()\n        failed_q = get_failed_queue()\n\n        # Action\n        q.enqueue('_non.importable.job')\n        self.assertEquals(q.count, 1)\n\n        w = Worker([q])\n        register_sentry(FakeSentry(), w)\n\n        w.work(burst=True)\n\n        # Postconditions\n        self.assertEquals(failed_q.count, 1)\n        self.assertEquals(q.count, 0)\n"
                }, 
                {
                    "type": "text", 
                    "name": "test_worker.py", 
                    "contents": "# -*- coding: utf-8 -*-\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport os\n\nfrom rq import get_failed_queue, Queue, Worker\nfrom rq.compat import as_text\nfrom rq.job import Job, Status\n\nfrom tests import RQTestCase, slow\nfrom tests.fixtures import (create_file, create_file_after_timeout, div_by_zero,\n                            say_hello)\nfrom tests.helpers import strip_microseconds\n\n\nclass TestWorker(RQTestCase):\n    def test_create_worker(self):\n        \"\"\"Worker creation.\"\"\"\n        fooq, barq = Queue('foo'), Queue('bar')\n        w = Worker([fooq, barq])\n        self.assertEquals(w.queues, [fooq, barq])\n\n    def test_work_and_quit(self):\n        \"\"\"Worker processes work, then quits.\"\"\"\n        fooq, barq = Queue('foo'), Queue('bar')\n        w = Worker([fooq, barq])\n        self.assertEquals(w.work(burst=True), False,\n                          'Did not expect any work on the queue.')\n\n        fooq.enqueue(say_hello, name='Frank')\n        self.assertEquals(w.work(burst=True), True,\n                          'Expected at least some work done.')\n\n    def test_worker_ttl(self):\n        \"\"\"Worker ttl.\"\"\"\n        w = Worker([])\n        w.register_birth()  # ugly: our test should only call public APIs\n        [worker_key] = self.testconn.smembers(Worker.redis_workers_keys)\n        self.assertIsNotNone(self.testconn.ttl(worker_key))\n        w.register_death()\n\n    def test_work_via_string_argument(self):\n        \"\"\"Worker processes work fed via string arguments.\"\"\"\n        q = Queue('foo')\n        w = Worker([q])\n        job = q.enqueue('tests.fixtures.say_hello', name='Frank')\n        self.assertEquals(w.work(burst=True), True,\n                          'Expected at least some work done.')\n        self.assertEquals(job.result, 'Hi there, Frank!')\n\n    def test_work_is_unreadable(self):\n        \"\"\"Unreadable jobs are put on the failed queue.\"\"\"\n        q = Queue()\n        failed_q = get_failed_queue()\n\n        self.assertEquals(failed_q.count, 0)\n        self.assertEquals(q.count, 0)\n\n        # NOTE: We have to fake this enqueueing for this test case.\n        # What we're simulating here is a call to a function that is not\n        # importable from the worker process.\n        job = Job.create(func=div_by_zero, args=(3,))\n        job.save()\n        data = self.testconn.hget(job.key, 'data')\n        invalid_data = data.replace(b'div_by_zero', b'nonexisting')\n        assert data != invalid_data\n        self.testconn.hset(job.key, 'data', invalid_data)\n\n        # We use the low-level internal function to enqueue any data (bypassing\n        # validity checks)\n        q.push_job_id(job.id)\n\n        self.assertEquals(q.count, 1)\n\n        # All set, we're going to process it\n        w = Worker([q])\n        w.work(burst=True)   # should silently pass\n        self.assertEquals(q.count, 0)\n        self.assertEquals(failed_q.count, 1)\n\n    def test_work_fails(self):\n        \"\"\"Failing jobs are put on the failed queue.\"\"\"\n        q = Queue()\n        failed_q = get_failed_queue()\n\n        # Preconditions\n        self.assertEquals(failed_q.count, 0)\n        self.assertEquals(q.count, 0)\n\n        # Action\n        job = q.enqueue(div_by_zero)\n        self.assertEquals(q.count, 1)\n\n        # keep for later\n        enqueued_at_date = strip_microseconds(job.enqueued_at)\n\n        w = Worker([q])\n        w.work(burst=True)  # should silently pass\n\n        # Postconditions\n        self.assertEquals(q.count, 0)\n        self.assertEquals(failed_q.count, 1)\n\n        # Check the job\n        job = Job.fetch(job.id)\n        self.assertEquals(job.origin, q.name)\n\n        # Should be the original enqueued_at date, not the date of enqueueing\n        # to the failed queue\n        self.assertEquals(job.enqueued_at, enqueued_at_date)\n        self.assertIsNotNone(job.exc_info)  # should contain exc_info\n\n    def test_custom_exc_handling(self):\n        \"\"\"Custom exception handling.\"\"\"\n        def black_hole(job, *exc_info):\n            # Don't fall through to default behaviour (moving to failed queue)\n            return False\n\n        q = Queue()\n        failed_q = get_failed_queue()\n\n        # Preconditions\n        self.assertEquals(failed_q.count, 0)\n        self.assertEquals(q.count, 0)\n\n        # Action\n        job = q.enqueue(div_by_zero)\n        self.assertEquals(q.count, 1)\n\n        w = Worker([q], exc_handler=black_hole)\n        w.work(burst=True)  # should silently pass\n\n        # Postconditions\n        self.assertEquals(q.count, 0)\n        self.assertEquals(failed_q.count, 0)\n\n        # Check the job\n        job = Job.fetch(job.id)\n        self.assertEquals(job.is_failed, True)\n\n    def test_cancelled_jobs_arent_executed(self):  # noqa\n        \"\"\"Cancelling jobs.\"\"\"\n\n        SENTINEL_FILE = '/tmp/rq-tests.txt'\n\n        try:\n            # Remove the sentinel if it is leftover from a previous test run\n            os.remove(SENTINEL_FILE)\n        except OSError as e:\n            if e.errno != 2:\n                raise\n\n        q = Queue()\n        job = q.enqueue(create_file, SENTINEL_FILE)\n\n        # Here, we cancel the job, so the sentinel file may not be created\n        assert q.count == 1\n        job.cancel()\n        assert q.count == 1\n\n        w = Worker([q])\n        w.work(burst=True)\n        assert q.count == 0\n\n        # Should not have created evidence of execution\n        self.assertEquals(os.path.exists(SENTINEL_FILE), False)\n\n    @slow  # noqa\n    def test_timeouts(self):\n        \"\"\"Worker kills jobs after timeout.\"\"\"\n        sentinel_file = '/tmp/.rq_sentinel'\n\n        q = Queue()\n        w = Worker([q])\n\n        # Put it on the queue with a timeout value\n        res = q.enqueue(create_file_after_timeout,\n                        args=(sentinel_file, 4),\n                        timeout=1)\n\n        try:\n            os.unlink(sentinel_file)\n        except OSError as e:\n            if e.errno == 2:\n                pass\n\n        self.assertEquals(os.path.exists(sentinel_file), False)\n        w.work(burst=True)\n        self.assertEquals(os.path.exists(sentinel_file), False)\n\n        # TODO: Having to do the manual refresh() here is really ugly!\n        res.refresh()\n        self.assertIn('JobTimeoutException', as_text(res.exc_info))\n\n    def test_worker_sets_result_ttl(self):\n        \"\"\"Ensure that Worker properly sets result_ttl for individual jobs.\"\"\"\n        q = Queue()\n        job = q.enqueue(say_hello, args=('Frank',), result_ttl=10)\n        w = Worker([q])\n        w.work(burst=True)\n        self.assertNotEqual(self.testconn._ttl(job.key), 0)\n\n        # Job with -1 result_ttl don't expire\n        job = q.enqueue(say_hello, args=('Frank',), result_ttl=-1)\n        w = Worker([q])\n        w.work(burst=True)\n        self.assertEqual(self.testconn._ttl(job.key), -1)\n\n        # Job with result_ttl = 0 gets deleted immediately\n        job = q.enqueue(say_hello, args=('Frank',), result_ttl=0)\n        w = Worker([q])\n        w.work(burst=True)\n        self.assertEqual(self.testconn.get(job.key), None)\n\n    def test_worker_sets_job_status(self):\n        \"\"\"Ensure that worker correctly sets job status.\"\"\"\n        q = Queue()\n        w = Worker([q])\n\n        job = q.enqueue(say_hello)\n        self.assertEqual(job.get_status(), Status.QUEUED)\n        self.assertEqual(job.is_queued, True)\n        self.assertEqual(job.is_finished, False)\n        self.assertEqual(job.is_failed, False)\n\n        w.work(burst=True)\n        job = Job.fetch(job.id)\n        self.assertEqual(job.get_status(), Status.FINISHED)\n        self.assertEqual(job.is_queued, False)\n        self.assertEqual(job.is_finished, True)\n        self.assertEqual(job.is_failed, False)\n\n        # Failed jobs should set status to \"failed\"\n        job = q.enqueue(div_by_zero, args=(1,))\n        w.work(burst=True)\n        job = Job.fetch(job.id)\n        self.assertEqual(job.get_status(), Status.FAILED)\n        self.assertEqual(job.is_queued, False)\n        self.assertEqual(job.is_finished, False)\n        self.assertEqual(job.is_failed, True)\n\n    def test_job_dependency(self):\n        \"\"\"Enqueue dependent jobs only if their parents don't fail\"\"\"\n        q = Queue()\n        w = Worker([q])\n        parent_job = q.enqueue(say_hello)\n        job = q.enqueue_call(say_hello, depends_on=parent_job)\n        w.work(burst=True)\n        job = Job.fetch(job.id)\n        self.assertEqual(job.get_status(), Status.FINISHED)\n\n        parent_job = q.enqueue(div_by_zero)\n        job = q.enqueue_call(say_hello, depends_on=parent_job)\n        w.work(burst=True)\n        job = Job.fetch(job.id)\n        self.assertNotEqual(job.get_status(), Status.FINISHED)\n\n    def test_get_current_job(self):\n        \"\"\"Ensure worker.get_current_job() works properly\"\"\"\n        q = Queue()\n        worker = Worker([q])\n        job = q.enqueue_call(say_hello)\n\n        self.assertEqual(self.testconn.hget(worker.key, 'current_job'), None)\n        worker.set_current_job_id(job.id)\n        self.assertEqual(\n            worker.get_current_job_id(),\n            as_text(self.testconn.hget(worker.key, 'current_job'))\n        )\n        self.assertEqual(worker.get_current_job(), job)\n"
                }
            ]
        }, 
        {
            "type": "text", 
            "name": "tox.ini", 
            "contents": "[tox]\nenvlist=py26,py27,py33,py34,pypy\n\n[testenv]\ncommands=py.test []\ndeps=\n    pytest\n    mock\n\n[testenv:py26]\ndeps=\n    pytest\n    unittest2\n    mock\n"
        }
    ]
}